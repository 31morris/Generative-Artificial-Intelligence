[
  {
    "title": "How Document Pre-processing affects Keyphrase Extraction Performance",
    "answer": "The dataset is composed of 244 scientific articles.",
    "evidence": [
      ". Scientific articles were selected from four different research areas as defined in the ACM classification, and were equally distributed into training (144 articles) and test (100 articles) sets. Gold standard keyphrases are composed of both author-assigned keyphrases collected from the original PDF files and reader-assigned keyphrases provided by student annotators",
      "Dataset and Preprocessing The SemEval-2010 benchmark dataset BIBREF0 is composed of 244 scientific articles collected from the ACM Digital Library (conference and workshop papers). The input papers ranged from 6 to 8 pages and were converted from PDF format to plain text using an off-the-shelf tool. The only preprocessing applied is a systematic dehyphenation at line breaks and removal of author-assigned keyphrases",
      ". This dataset is composed of documents (scientific articles) that were automatically converted from PDF format to plain text. As a result, most documents contain irrelevant pieces of text (e.g. muddled sentences, tables, equations, footnotes) that require special handling, so as to not hinder the performance of keyphrase extraction systems",
      ". The input papers ranged from 6 to 8 pages and were converted from PDF format to plain text using an off-the-shelf tool. The only preprocessing applied is a systematic dehyphenation at line breaks and removal of author-assigned keyphrases. Scientific articles were selected from four different research areas as defined in the ACM classification, and were equally distributed into training (144 articles) and test (100 articles) sets",
      ". To answer this question, we take another step beyond content filtering and further abridge the input text from level 3 preprocessed documents using an unsupervised summarization technique. More specifically, we keep the title and abstract intact, as they are the two most keyphrase dense parts within scientific articles BIBREF4 , and select only the most content bearing sentences from the remaining contents"
    ]
  },
  {
    "title": "Comparative Studies of Detecting Abusive Language on Twitter",
    "answer": "The authors provide examples of tweets (2) labeled as abusive due to the use of vulgar language, but the intention behind the language can be better understood when considering the context of the preceding tweet (1).",
    "evidence": [
      ". As shown in the examples below, (2) is labeled abusive due to the use of vulgar language. However, the intention of the user can be better understood with its context tweet (1). (1) I hate when I'm sitting in front of the bus and somebody with a wheelchair get on. INLINEFORM0 (2) I hate it when I'm trying to board a bus and there's already an as**ole on it. Similarly, context tweet (3) is important in understanding the abusive tweet (4), especially in identifying the target of the malice",
      ".e. Facebook, Twitter) have utilized multiple resources—artificial intelligence, human reviewers, user reporting processes, etc.—in effort to censor offensive language, yet it seems nearly impossible to successfully resolve the issue BIBREF7 , BIBREF8 . The major reason of the failure in abusive language detection comes from its subjectivity and context-dependent characteristics BIBREF9",
      ". Recently, Hate and Abusive Speech on Twitter, a dataset much greater in size and reliability, has been released. However, this dataset has not been comprehensively studied to its potential. In this paper, we conduct the first comparative study of various learning models on Hate and Abusive Speech on Twitter, and discuss the possibility of using additional features and context data for improvements",
      "Introduction Abusive language refers to any type of insult, vulgarity, or profanity that debases the target; it also can be anything that causes aggravation BIBREF0 , BIBREF1 . Abusive language is often reframed as, but not limited to, offensive language BIBREF2 , cyberbullying BIBREF3 , othering language BIBREF4 , and hate speech BIBREF5 . Recently, an increasing number of users have been subjected to harassment, or have witnessed offensive behaviors online BIBREF6 . Major social media companies (i.e",
      "Feature Extension While manually analyzing the raw dataset, we noticed that looking at the tweet one has replied to or has quoted, provides significant contextual information. As humans can better understand a tweet with the reference of its context, our assumption is that computers also benefit from taking context tweets into account in detecting abusive language. As shown in the examples below, (2) is labeled abusive due to the use of vulgar language"
    ]
  },
  {
    "title": "Prepositional Attachment Disambiguation Using Bilingual Parsing and Alignments",
    "answer": "Enforcing agreement between parse trees across different languages involves aligning words in one language to corresponding words in another language and expecting dependency edges to correspond between the two languages, with the possibility of an edge in one language corresponding to a path in the other language.",
    "evidence": [
      ". The DD algorithm then tries to enforce agreement between the two parse trees subject to the given alignments. Let us take a closer look at what we mean by agreement between the two parse trees. Essentially, if we have two words in the English sentence denoted by i and i', aligned to words j and j' in the parallel Hindi sentence respectively, we can expect a dependency edge between i and i' in the English parse tree to correspond to an edge between j and j' in the Hindi parse tree, and vice versa",
      ". Also, in order to accommodate structural diversity in languages BIBREF1 , we can expect an edge in the parse tree in one language to correspond to more than one edge, or rather, a path, in the other language parse tree. This has been captured in the examples in figures FIGREF6 (A) and FIGREF6 (B)",
      ". The idea is to correct the PP-attachments for a sentence with the help of alignments from parallel data in another language. The novelty of our work lies in the formulation of the problem into a dual decomposition based algorithm that enforces agreement between the parse trees from two languages as a constraint",
      ". The novelty of our work lies in the formulation of the problem into a dual decomposition based algorithm that enforces agreement between the parse trees from two languages as a constraint. Experiments were performed on the English-Hindi language pair and the performance improved by 10% over the baseline, where the baseline is the attachment predicted by the MSTParser model trained for English.",
      ". We train two separate parser models for English and Hindi each, using the MSTParser, and make use of these models in the inferencing step. The input to the algorithm is a parallel English-Hindi sentence pair, with its word alignments given. We first obtain the predicted parse trees for the English and Hindi sentences from the respective trained parser models as an initialsiation step. The DD algorithm then tries to enforce agreement between the two parse trees subject to the given alignments"
    ]
  },
  {
    "title": "NeuronBlocks: Building Your NLP DNN Models Like Playing Lego",
    "answer": "The most commonly used components of deep neural networks in NeuronBlocks are categorized into several groups according to their functions and encapsulated into standard and reusable blocks with a consistent interface.",
    "evidence": [
      "Design  The Neuronblocks is built on PyTorch. The overall framework is illustrated in Figure FIGREF16 . It consists of two layers: the Block Zoo and the Model Zoo. In Block Zoo, the most commonly used components of deep neural networks are categorized into several groups according to their functions. Within each category, several alternative components are encapsulated into standard and reusable blocks with a consistent interface",
      ". The bottom layer consists of a suite of reusable and standard components, which can be adopted as building blocks to construct networks with complex architecture. By following the interface guidelines, users can also contribute to this gallery of components with their own modules. The technical contributions of NeuronBlocks are summarized into the following three aspects.",
      ". Table TABREF28 shows the results on CoNLL-2003 Englist testb dataset, with 12 different combinations of network layers/blocks, such as word/character embedding, CNN/LSTM and CRF. The results suggest that the flexible combination of layers/blocks in NeuronBlocks can easily reproduce the performance of original models, with comparative or slightly better performance.",
      ". An NLP toolkit for DNN models with both generality and flexibility can greatly improve the productivity of engineers by saving their learning cost and guiding them to find optimal solutions to their tasks. In this paper, we introduce NeuronBlocks, a toolkit encapsulating a suite of neural network modules as building blocks to construct various DNN models with complex architecture. This toolkit empowers engineers to build, train, and test various NLP models through simple configuration of JSON files",
      ". Figure FIGREF19 shows an example of how to specify a model architecture using the blocks in NeuronBlocks. To be more specific, it consists of a list of layers/blocks to construct the architecture, where the blocks are supplied in the gallery of Block Zoo. In this part, the model optimizer as well as all other training hyper parameters are indicated."
    ]
  },
  {
    "title": "Evaluating Tag Recommendations for E-Book Annotation Using a Semantic Similarity Metric",
    "answer": "They evaluated 19 different algorithms to recommend tags for annotating e-books, including popularity-based and similarity-based approaches, as well as a cross-source algorithm.",
    "evidence": [
      ". Thus, these results also suggest to investigate beyond-accuracy metrics as done in Section SECREF17 . Figure FIGREF11 shows the accuracy results of the four hybrid approaches. By combining the best three popularity-based approaches, we outperform all of the initially evaluated popularity algorithms (i.e., INLINEFORM0 for INLINEFORM1 ). On the contrary, the combination of the two best performing similarity-based approaches INLINEFORM2 and INLINEFORM3 does not yield better accuracy",
      ".e., we do not know which user has assigned a tag), we do not implement other types of algorithms such as collaborative filtering BIBREF8 . In total, we evaluate 19 different algorithms to recommend tags for annotating e-books. We recommend the most frequently used tags in the dataset, which is a common strategy for tag recommendations BIBREF9 . That is, a most popular INLINEFORM0 approach for editor tags and a most popular INLINEFORM1 approach for Amazon search terms",
      ". We again use a round-robin strategy to combine both data sources (= INLINEFORM3 and INLINEFORM4 ). We use the previously mentioned cross-source algorithm BIBREF13 to construct four hybrid recommendation approaches. In this case, tags are favored that are recommended by more than one algorithm",
      ".e., the Amazon review keywords). Over the last four years, there have been several notable publications in the area of applying deep learning to uncover semantic relationships between textual content (e.g., by learning word embeddings with Word2Vec BIBREF16 , BIBREF17 )",
      ". We further define INLINEFORM2 , a hybrid approach that combines the three popularity-based methods of INLINEFORM3 and the two similarity-based approaches of INLINEFORM4 . Finally, we define INLINEFORM5 as a hybrid approach that uses the best performing popularity-based and the best performing similarity-based approach (see Figure FIGREF11 in Section SECREF4 for more details about the particular algorithm combinations)."
    ]
  },
  {
    "title": "Harnessing Cognitive Features for Sarcasm Detection",
    "answer": "The state-of-the-art system with textual sarcasm features alone is compared to the output of the MILR classifier with the complete set of features, and the results show that the improvement obtained is statistically significant.",
    "evidence": [
      "Results Table TABREF17 shows the classification results considering various feature combinations for different classifiers and other systems. These are: Unigram (with principal components of unigram feature vectors), Sarcasm (the feature-set reported by joshi2015harnessing subsuming unigram features and features from other reported systems) Gaze (the simple and complex cognitive features we introduce, along with readability and word count features), and Gaze+Sarcasm (the complete set of features)",
      ". Consider a scenario where an online reviewer gives a negative opinion about a movie through sarcasm: “This is the kind of movie you see because the theater has air conditioning”. It is difficult for an automatic sentiment analyzer to assign a rating to the movie and, in the absence of any other information, such a system may not be able to comprehend that prioritizing the air-conditioning facilities of the theater over the movie experience indicates a negative sentiment towards the movie",
      ". We train our classifier with 100%, 90%, 80% and 70% of the training data with our whole feature set, and the feature combination from joshi2015harnessing. The goodness of our system is demonstrated by improvements in F-score and Kappa statistics, shown in Figure FIGREF22 . We further analyze the importance of features by ranking the features based on (a) Chi squared test, and (b) Information Gain test, using Weka's attribute selection module",
      ". To see if the improvement obtained is statistically significant over the state-of-the art system with textual sarcasm features alone, we perform McNemar test. The output of the SVM classifier using only linguistic features used for sarcasm detection by joshi2015harnessing and the output of the MILR classifier with the complete set of features are compared, setting threshold INLINEFORM0 . There was a significant difference in the classifier's accuracy with p(two-tailed) = 0.02 with an odds-ratio of 1",
      ". However, it has been quite difficult to solve such a problem with traditional NLP tools and techniques. This is apparent from the results reported by the survey from DBLP: The following discussion brings more insights into this. Consider a scenario where an online reviewer gives a negative opinion about a movie through sarcasm: “This is the kind of movie you see because the theater has air conditioning”"
    ]
  },
  {
    "title": "Arabic Offensive Language on Twitter: Analysis and Experiments",
    "answer": "Our methodology does not use a seed list of offensive words, which prevents it from being biased by topic, target, or dialect.",
    "evidence": [
      ". We showed that using static embeddings produced a competitive results on the dataset. For future work, we plan to pursue several directions. First, we want explore target specific offensive language, where attacks against an entity or a group may employ certain expressions that are only offensive within the context of that target and completely innocuous otherwise. Second, we plan to examine the effectiveness of cross dialectal and cross lingual learning of offensive language.",
      ". Though the use of offensive language does not necessitate the appearance of the vocative particle, the particle does not favor any specific offensive expressions and greatly improves our chances of finding offensive tweets. It is clear, the dataset is more biased toward positive class. Using the dataset for real-life application may require de-biasing it by boosting negative class or random sampling additional data from Twitter BIBREF22",
      ". We introduce a method for building an offensive dataset that is not biased by topic, dialect, or target. We produce the largest Arabic dataset to date with special tags for vulgarity and hate speech. Next, we analyze the dataset to determine which topics, dialects, and gender are most associated with offensive tweets and how Arabic speakers use offensive language. Lastly, we conduct a large battery of experiments to produce strong results (F1 = 79.7) on the dataset using Support Vector Machine techniques.",
      ". Since our methodology does not use a seed list of offensive words, it is not biased by topic, target, or dialect. Using our methodology, we tagged 10,000 Arabic tweet dataset for offensiveness, where offensive tweets account for roughly 19% of the tweets. Further, we labeled tweets as vulgar or hate speech. To date, this is the largest available dataset, which we plan to make publicly available along with annotation guidelines",
      "Introduction Disclaimer: Due to the nature of the paper, some examples contain highly offensive language and hate speech. They don't reflect the views of the authors in any way, and the point of the paper is to help fight such speech. Much recent interest has focused on the detection of offensive language and hate speech in online social media. Such language is often associated with undesirable online behaviors such as trolling, cyberbullying, online extremism, political polarization, and propaganda"
    ]
  },
  {
    "title": "A Bayesian Model of Multilingual Unsupervised Semantic Role Induction",
    "answer": "An individual model in the joint Bayesian model consists of a base monolingual model, individual models for each language, and additional latent variables that capture alignments between roles across languages.",
    "evidence": [
      ". The sample counts and the priors are then used to calculate the MAP estimate of the model parameters. For the monolingual model, the role at a given position is sampled as:  DISPLAYFORM0   where the subscript INLINEFORM0 refers to all the variables except at position INLINEFORM1 , INLINEFORM2 refers to the variables in all the training instances except the current one, and INLINEFORM3 refers to all the model parameters. The above integral has a closed form solution due to Dirichlet-multinomial conjugacy",
      ". Our joint Bayesian model consists of individual models for each language plus additional latent variables that capture alignments between roles across languages. Because it is a generative Bayesian model, we can do evaluations in a variety of scenarios just by varying the inference procedure, without changing the model, thereby comparing the scenarios directly",
      "Monolingual Model We use the Bayesian model of garg2012unsupervised as our base monolingual model. The semantic roles are predicate-specific. To model the role ordering and repetition preferences, the role inventory for each predicate is divided into Primary and Secondary roles as follows: For example, the complete role sequence in a frame could be: INLINEFORM0 INLINEFORM1 , INLINEFORM2 , INLINEFORM3 , INLINEFORM4 , INLINEFORM5 , INLINEFORM6 , INLINEFORM7 , INLINEFORM8 INLINEFORM9",
      ". or to a modifier role like INLINEFORM24 , INLINEFORM25 , etc. garg2012unsupervised reported that, in practice, PRs mostly get mapped to core roles and SRs to modifier roles, which conforms to the linguistic motivations for this distinction. Figure FIGREF16 illustrates two copies of the monolingual model, on either side of the crosslingual latent variables. The generative process is as follows:  All the multinomial and binomial distributions have symmetric Dirichlet and beta priors respectively",
      ". The multilingual model is deficient, since the aligned roles are being generated twice. Ideally, we would like to add the CLV as additional conditioning variables in the monolingual models. The new joint probability can be written as equation UID11 (Figure FIGREF7 ), which can be further decomposed following the decomposition of the monolingual model in Figure FIGREF7"
    ]
  },
  {
    "title": "Hierarchical Transformers for Long Document Classification",
    "answer": "The Transformer layer is used in both RoBERT and ToBERT, and it is shown to be effective in capturing long distance relationships between words in a sequence, whereas the RNN layer is used in RoBERT, which is an alternative to ToBERT.",
    "evidence": [
      "Method ::: Transformer over BERT Given that Transformers' edge over recurrent networks is their ability to effectively capture long distance relationships between words in a sequence BIBREF0, we experiment with replacing the LSTM recurrent layer in favor of a small Transformer model (2 layers of transformer building block containing self-attention, fully connected, etc.)",
      ". We segment the input into smaller chunks and feed each of them into the base model. Then, we propagate each output through a single recurrent layer, or another transformer, followed by a softmax activation. We obtain the final classification decision after the last segment has been consumed. We show that both BERT extensions are quick to fine-tune and converge after as little as 1 epoch of training on a small, domain-specific data set",
      ". Then, we use either a recurrent LSTM BIBREF10 network, or another Transformer, to perform the actual classification. We call these techniques Recurrence over BERT (RoBERT) and Transformer over BERT (ToBERT). Given that these models introduce a hierarchy of representations (segment-wise and document-wise), we refer to them as Hierarchical Transformers. To the best of our knowledge, no attempt has been done before to use the Transformer architecture for classification of such long sequences",
      "Method ::: BERT Because our work builds heavily upon BERT, we provide a brief summary of its features. BERT is built upon the Transformer architecture BIBREF0, which uses self-attention, feed-forward layers, residual connections and layer normalization as the main building blocks",
      ". BERT has been shown to beat the state-of-the-art performance on 11 tasks with no modifications to the model architecture, besides adding a task-specific output layer BIBREF1. We follow same procedure suggested in BIBREF1 for our tasks. FIGREF8 shows the BERT model for classification. We obtain two kinds of representation from BERT: pooled output from last transformer block, denoted by H, and posterior probabilities, denoted by P. There are two variants of BERT - BERT-Base and BERT-Large"
    ]
  },
  {
    "title": "Impact of Batch Size on Stopping Active Learning for Text Classification",
    "answer": "The context does not explicitly mention the downstream tasks being evaluated.",
    "evidence": [
      ". An important parameter used by stopping methods is what window size of earlier iterations to consider in making the stopping decision. Our results indicate that making the window size smaller helps to mitigate the degradation in stopping method performance that occurs with larger batch sizes.",
      "Acknowledgment This work was supported in part by The College of New Jersey Support of Scholarly Activities (SOSA) program, by The College of New Jersey Mentored Undergraduate Summer Experience (MUSE) program, and by usage of The College of New Jersey High Performance Computing System.",
      "Conclusion Active learning has the potential to significantly reduce annotation costs. Two important considerations in the active learning process are when to stop the iterative process of asking for more labeled data and how large of a batch size to use when asking for additional labels during each iteration. We found that stopping methods degrade in performance when larger batch sizes are used",
      ". While past work has shown that larger batch sizes decrease learning efficiency from a learning curve perspective, it remains an open question how batch size impacts methods for stopping active learning. We find that large batch sizes degrade the performance of a leading stopping method over and above the degradation that results from reduced learning efficiency",
      ". We found that stopping methods degrade in performance when larger batch sizes are used. The degradation in performance is larger than the amount that can be explained due to the degradation in learning efficiency that results from using larger batch sizes. An important parameter used by stopping methods is what window size of earlier iterations to consider in making the stopping decision"
    ]
  },
  {
    "title": "Generating Personalized Recipes from Historical User Preferences",
    "answer": "They get the recipes from cooking websites.",
    "evidence": [
      ". These users with limited knowledge cannot rely on existing recipe generation approaches that focus on creating coherent recipes given all ingredients and a recipe name BIBREF0. Such models do not address issues of personal preference (e.g. culinary tastes, garnish choices) and incomplete recipe details. We propose to approach both problems via personalized generation of plausible, user-specific recipes using user preferences extracted from previously consumed recipes",
      ". Qualitative Analysis: We present sample outputs for a cocktail recipe in tab:samplerecipes, and additional recipes in the appendix. Generation quality progressively improves from generic baseline output to a blended cocktail produced by our best performing model. Models attending over prior recipes explicitly reference ingredients. The Prior Name model further suggests the addition of lemon and mint, which are reasonably associated with previously consumed recipes like coconut mousse and pork skewers",
      ". We evaluate only on recipes not in the training set. We manually construct a list of 58 cooking techniques from 384 cooking actions collected by BIBREF15; the most common techniques (bake, combine, pour, boil) account for 36.5% of technique mentions. We approximate technique adherence via string match between the recipe text and technique list.",
      "Introduction In the kitchen, we increasingly rely on instructions from cooking websites: recipes. A cook with a predilection for Asian cuisine may wish to prepare chicken curry, but may not know all necessary ingredients apart from a few basics. These users with limited knowledge cannot rely on existing recipe generation approaches that focus on creating coherent recipes given all ingredients and a recipe name BIBREF0. Such models do not address issues of personal preference (e.g",
      ". Models attending over prior recipes explicitly reference ingredients. The Prior Name model further suggests the addition of lemon and mint, which are reasonably associated with previously consumed recipes like coconut mousse and pork skewers. Personalization: To measure personalization, we evaluate how closely the generated text corresponds to a particular user profile"
    ]
  },
  {
    "title": "Pyramidal Recurrent Unit for Language Modeling",
    "answer": "The validation sets of the PTB and the WT-2 datasets were used to report word-level perplexities.",
    "evidence": [
      "Ablation studies In this section, we provide a systematic analysis of our design choices. Our training methodology is the same as described in Section SECREF19 For a thorough understanding of our design choices, we use a language model with a single layer of PRU and fix the size of embedding and hidden layers to 600. The word-level perplexities are reported on the validation sets of the PTB and the WT-2 datasets",
      "Acknowledgments This research was supported by NSF (IIS 1616112, III 1703166), Allen Distinguished Investigator Award, and gifts from Allen Institute for AI, Google, Amazon, and Bloomberg. We are grateful to Aaron Jaech, Hannah Rashkin, Mandar Joshi, Aniruddha Kembhavi, and anonymous reviewers for their helpful comments.",
      ". With the advanced dropouts, the performance of PRUs improves by about 4 points on the PTB dataset and 7 points on the WT-2 dataset. This further improves with finetuning on the PTB (about 2 points) and WT-2 (about 1 point) datasets. For similar number of parameters, the PRU with standard dropout outperforms most of the state-of-the-art methods by large margin on the PTB dataset (e.g. RAN BIBREF7 by 16 points with 4M less parameters, QRNN BIBREF33 by 16 points with 1M more parameters, and NAS BIBREF31 by 1",
      ". Our model uses 3-layers of PRU with an embedding size of 400. The number of parameters learned by state-of-the-art methods vary from 18M to 66M with majority of the methods learning about 22M to 24M parameters on the PTB dataset. For a fair comparison with state-of-the-art methods, we fix the model size to 19M and vary the value of INLINEFORM0 and hidden layer sizes so that total number of learned parameters is similar across different configurations",
      ". (4) Max pool: We select the maximum value in the kernel window INLINEFORM7 . Table TABREF45 compares the performance of the PRU with different sampling methods. Average pooling performs the best while skipping give comparable performance. Both of these methods enable the network to learn richer word representations while representing the input vector in different forms, thus delivering higher performance"
    ]
  },
  {
    "title": "Emotion Detection in Text: Focusing on Latent Representation",
    "answer": "The GRU model captures the sequential nature of the text and the context, which are lost in traditional ML models such as the bag of words model or lexicon features.",
    "evidence": [
      ". Then a max-pooling was used to extract the most important features form the GRU output and an average-pooling layer was used to considers all features to create a representation for the text as a whole. These partial representations are then were concatenated to create out final hidden representation. For classification, the output of the concatenation is passed to a dense classification layer with 70 nodes along with a dropout layer with a rate of 50% to prevent over-fitting",
      ". These methods, therefore, are not sufficient to create an applicable and generalizable emotion detection methodology. Understanding these limitations, we present a new network based on a bidirectional GRU model to show that capturing more meaningful information from text can significantly improve the performance of these models. The results show significant improvement with an average of 26.8 point increase in F-measure on our test data and 38.6 increase on the totally new dataset.",
      ". As mentioned in the Introduction, Recurrent Neural Networks (RNNs) have been shown to perform well for the verity of tasks in NLP, especially classification tasks. And as our goal was to capture more information about the context and sequential nature of the text, we decided to use a model based on bidirectional RNN, specifically a bidirectional GRU network to analyze the tweets",
      ". You can see the plot diagram of the model in Figure FIGREF6 . The first layer consists of an embedding lookup layer that will not change during training and will be used to convert each term to its corresponding embedding vector. In our experiments, we tried various word embedding models but saw little difference in their performance. Here we report the results for two which had the best performance among all, ConceptNet Numberbatch BIBREF35 and fastText BIBREF36 both had 300 dimensions",
      ". During the process of creating the feature set, in these methods, some of the most important information in the text such as the sequential nature of the data, and the context will be lost. Considering the complexity of the task, and the fact that these models lose a lot of information by using simpler models such as the bag of words model (BOW) or lexicon features, these attempts lead to methods which are not reusable and generalizable"
    ]
  },
  {
    "title": "ReviewQA: a relational aspect-based opinion reading dataset",
    "answer": "The hotel reviews are extracted from the TripAdvisor website.",
    "evidence": [
      "Original data We used a set of reviews extracted from the TripAdvisor website and originally proposed in BIBREF10 and BIBREF11 . This corpus is available at http://www.cs.virginia.edu/~hw5x/Data/LARA/TripAdvisor/TripAdvisorJson.tar.bz2. Each review comes with the name of the associated hotel, a title, an overall rating, a comment and a list of rated aspects. From 0 to 7 aspects, among value, room, location, cleanliness, check-in/front desk, service, business service, can possibly be rated in a review",
      ". To set up this challenge, we propose to leverage on a hotel reviews corpus that requires reasoning skills to answer natural language questions. The reviews we used have been extracted from TripAdvisor and originally proposed in BIBREF10 , BIBREF11 . In the original data, each review comes with a set of rated aspects among the seventh available: Business service, Check in / Front Desk, Cleanliness, Location, Room, Sleep Quality, Value and for all the reviews an Overall rating",
      ". From 0 to 7 aspects, among value, room, location, cleanliness, check-in/front desk, service, business service, can possibly be rated in a review. Figure FIGREF8 displays a review extracted from this dataset.",
      ". The second step is the ability of reasoning with the relevant information extracted from a document. To set up this challenge, we propose to leverage on a hotel reviews corpus that requires reasoning skills to answer natural language questions. The reviews we used have been extracted from TripAdvisor and originally proposed in BIBREF10 , BIBREF11",
      ". In this context, we introduce a set of reasoning questions types over the relationships between aspects. We propose ReviewQA, a dataset of natural language questions over hotel reviews. These questions are divided into 8 groups, regarding the competency required to be answered. In this section, we describe each task and the process followed to generate this dataset."
    ]
  },
  {
    "title": "Fine-Grained Named Entity Recognition using ELMo and Wikidata",
    "answer": "They combine a state-of-the-art deep learning model (ELMo) with an expansive knowledge base (Wikidata) to address the issues in entity detection systems.",
    "evidence": [
      ". Our work attempts to address these issues, in part, by combining state-of-the-art deep learning models (ELMo) with an expansive knowledge base (Wikidata). Using our framework, we cross-validate our model on the 112 fine-grained entity types based on the hierarchy given from the Wiki(gold) dataset.",
      "Method Over the few past years, the emergence of deep neural networks has fundamentally changed the design of entity detection systems. Consequently, recurrent neural networks (RNN) have found popularity in the field since they are able to learn long term dependencies of sequential data. The recent success of neural network based architectures principally comes from its deep structure. Training a deep neural network, however, is a difficult problem due to vanishing or exploding gradients",
      ". Most existing studies consider NER and entity linking as two separate tasks, whereas we try to combine the two. It has been shown that one can significantly increase the semantic information carried by a NER system when we successfully linking entities from a deep learning method to the related entities from a knowledge base BIBREF26 , BIBREF27 . For the Wikidata linking element, we recognize that the lookup will be constrained by the most common lookup name for each entity",
      ". yogatama2015embedding proposed an embedding based model where user-defined features and labels were embedded into a low dimensional feature space to facilitate information sharing among labels. Shimaoka et al. shimaoka2016attentive proposed an attentive neural network model which used long short-term memory (LSTMs) to encode the context of the entity, then used an attention mechanism to allow the model to focus on relevant expressions in the entity mention's context",
      ". There are a number of knowledge bases that provide a background repository for entity classification of this type. For this study, we use Wikidata, which can be seen diagrammatically in Figure FIGREF12 . Systems such as DeepType BIBREF25 integrate symbolic information into the reasoning process of a neural network with a type system and show state-of-the-art performances for EL. They do not, however, quote results on Wiki(gold) so a direct comparison is difficult"
    ]
  },
  {
    "title": "Bringing Structure into Summaries: Crowdsourcing a Benchmark Corpus of Concept Maps",
    "answer": "The authors define a concept map as a labeled graph showing concepts as nodes and relationships between them as edges.",
    "evidence": [
      "Task Concept-map-based MDS is defined as follows: Given a set of related documents, create a concept map that represents its most important content, satisfies a specified size limit and is connected. We define a concept map as a labeled graph showing concepts as nodes and relationships between them as edges. Labels are arbitrary sequences of tokens taken from the documents, making the summarization task extractive. A concept can be an entity, abstract idea, event or activity, designated by its unique label",
      ". A representation that is more in line with observed user behavior is a concept map BIBREF5 , a labeled graph showing concepts as nodes and relationships between them as edges (Figure FIGREF2 ). Introduced in 1972 as a teaching tool BIBREF6 , concept maps have found many applications in education BIBREF7 , BIBREF8 , for writing assistance BIBREF9 or to structure information repositories BIBREF10 , BIBREF11",
      ". As these decisions highly depend upon each other and require context, we decided to use expert annotators rather than crowdsource the subtasks. Annotators were given the topic description and the most important, ranked propositions. Using a simple annotation tool providing a visualization of the graph, they could connect the propositions step by step. They were instructed to reach a size of 25 concepts, the recommended maximum size for a concept map BIBREF6",
      ". Introduced in 1972 as a teaching tool BIBREF6 , concept maps have found many applications in education BIBREF7 , BIBREF8 , for writing assistance BIBREF9 or to structure information repositories BIBREF10 , BIBREF11 . For summarization, concept maps make it possible to represent a summary concisely and clearly reveal relationships",
      ". For concept map generation, one corpus with human-created summary concept maps for student essays has been created BIBREF28 . In contrast to our corpus, it only deals with single documents, requires a two orders of magnitude smaller amount of compression of the input and is not publicly available . Other types of information representation that also model concepts and their relationships are knowledge bases, such as Freebase BIBREF29 , and ontologies"
    ]
  },
  {
    "title": "CoVoST: A Diverse Multilingual Speech-To-Text Translation Corpus",
    "answer": "The quality of the data is empirically evaluated through quality checks, manual inspection of translations with low scores, and measurement of perplexity using a language model.",
    "evidence": [
      ". This makes the resulting evaluation set closer to real-world scenarios and more challenging. We run the same quality checks for TT as for CoVoST but we do not find poor quality translations according to our criteria. Finally, we report the overlap between CoVo transcripts and TT sentences in Table TABREF5. We found a minimal overlap, which makes the TT evaluation set a suitable additional test set when training on CoVoST.",
      ". We applied this method to these three language pairs only as we are confident about the quality of the corresponding systems. Translations with a score that was too low were manually inspected and sent back to the translators when needed. 2) We manually inspected examples where the source transcript was identical to the translation. 3) We measured the perplexity of the translations using a language model trained on a large amount of clean monolingual data BIBREF14",
      "Data Analysis ::: Basic Statistics Basic statistics for CoVoST and TT are listed in Table TABREF2 including (unique) sentence counts, speech durations, speaker demographics (partially available) as well as vocabulary and token statistics (based on Moses-tokenized sentences by sacreMoses) on both transcripts and translations",
      "Baseline Results ::: Multi-Speaker Evaluation In CoVoST, large portion of transcripts are covered by multiple speakers with different genders, accents and age groups. Besides the standard corpus-level BLEU scores, we also want to evaluate model output variance on the same content (transcript) but different speakers",
      ". We collect (speech, transcript, English translation) triplets for the 5 languages and do not include those whose speech has a broken URL or is not CC licensed. We further filter these samples by sentence lengths (minimum 4 words including punctuations) to reduce the portion of short sentences. This makes the resulting evaluation set closer to real-world scenarios and more challenging. We run the same quality checks for TT as for CoVoST but we do not find poor quality translations according to our criteria"
    ]
  },
  {
    "title": "Improving the Performance of Neural Machine Translation Involving Morphologically Rich Languages",
    "answer": "A collection of samples of about 100 sentences were taken from the test set results for comparison, and a randomized selection of the translation results was included to ensure the objectivity of evaluation.",
    "evidence": [
      "Human Evaluation To ensure that the increase in BLEU score correlated to actual increase in performance of translation, human evaluation metrics like adequacy, precision and ranking values (between RNNSearch and RNNMorph outputs) were estimated in Table TABREF30 . A group of 50 native people who were well-versed in both English and Tamil languages acted as annotators for the evaluation. A collection of samples of about 100 sentences were taken from the test set results for comparison",
      ". This enabled selective lookup to the source sentence during decoding and is known as the attention mechanism BIBREF27 . The attention mechanism was further analysed by Luong et al. luong2015effective where they made a distinction between global and local attention by means of AER scores of the attention vectors. A Gaussian distribution and a monotonic lookup were used to facilitate the corresponding local source sentence look-up.",
      ". This set included a randomized selection of the translation results to ensure the objectivity of evaluation. Fluency and adequacy results for the RNNMorph results are tabulated. Adequacy rating was calculated on a 5-point scale of how much of the meaning is conveyed by the translation (All, Most, Much, Little, None). The fluency rating was calculated based on grammatical correctness on a 5-point scale of (Flawless, Good, Non-native, Disfluent, Incomprehensive)",
      ". A group of 50 native people who were well-versed in both English and Tamil languages acted as annotators for the evaluation. A collection of samples of about 100 sentences were taken from the test set results for comparison. This set included a randomized selection of the translation results to ensure the objectivity of evaluation. Fluency and adequacy results for the RNNMorph results are tabulated",
      "Results and Discussion The BLEU metric parameters (modified 1-gram, 2-gram, 3-gram and 4-gram precision values) and human evaluation metrics of adequacy, fluency and relative ranking values were used to evaluate the performance of the models."
    ]
  },
  {
    "title": "Multi-task Pairwise Neural Ranking for Hashtag Segmentation",
    "answer": "The dataset of hashtags is sourced from the Stanford Sentiment Analysis Dataset.",
    "evidence": [
      "Hashtag Segmentation Data We use two datasets for experiments (Table TABREF29 ): (a) STAN INLINEFORM0 , created by BIBREF10 BansalBV15, which consists of 1,108 unique English hashtags from 1,268 randomly selected tweets in the Stanford Sentiment Analysis Dataset BIBREF36 along with their crowdsourced segmentations and our additional corrections; and (b) STAN INLINEFORM1 , our new expert curated dataset, which includes all 12,594 unique English hashtags and their associated tweets from the same Stanford",
      ". We also constructed a larger and more curated dataset for analyzing and benchmarking hashtag segmentation methods. We demonstrated that hashtag segmentation helps with downstream tasks such as sentiment analysis. Although we focused on English hashtags, our pairwise ranking approach is language-independent and we intend to extend our toolkit to languages other than English as future work.",
      ".g., #iloveu4eva) BIBREF8 , BIBREF9 , which hinders their understanding. The goal of our study is to build efficient methods for automatically splitting a hashtag into a meaningful word sequence. Our contributions are: Our new dataset includes segmentation for 12,594 unique hashtags and their associated tweets annotated in a multi-step process for higher quality than the previous dataset of 1,108 hashtags BIBREF10",
      "Dataset BIBREF36 along with their crowdsourced segmentations and our additional corrections; and (b) STAN INLINEFORM1 , our new expert curated dataset, which includes all 12,594 unique English hashtags and their associated tweets from the same Stanford dataset",
      ". However, the semantic content of hashtags is not straightforward to infer as these represent ad-hoc conventions which frequently include multiple words joined together and can include abbreviations and unorthodox spellings. We build a dataset of 12,594 hashtags split into individual segments and propose a set of approaches for hashtag segmentation by framing it as a pairwise ranking problem between candidate segmentations. Our novel neural approaches demonstrate 24"
    ]
  },
  {
    "title": "Extractive Summarization of EHR Discharge Notes",
    "answer": "The datasets used were the annotated history of present illness notes, discharge summaries, and all MIMIC notes.",
    "evidence": [
      ". Table TABREF16 compares dev set performance of the model using various pretrained word embeddings, with and without character embeddings, and with pretrained versus learned word embeddings. The first row in each section is the performance of the model architecture described in the methods section for comparison",
      "Labeling History of Present Illness Notes We developed a classifier to label topics in the history of present illness (HPI) notes, including demographics, diagnosis history, and symptoms/signs, among others. A random sample of 515 history of present illness notes was taken, and each of the notes was manually annotated by one of eight annotators using the software Multi-document Annotation Environment (MAE) BIBREF20",
      ". The first row in each section is the performance of the model architecture described in the methods section for comparison. Models using word embeddings trained on the discharge summaries performed better than word embeddings trained on all MIMIC notes, likely because the discharge summary word embeddings better captured word use in discharge summaries alone",
      ". We evaluated the impact of character embeddings, the choice of pretrained w2v embeddings, and the addition of learned word embeddings on model performance on the dev set. We report performance of the best performing model on the test set. Table TABREF16 compares dev set performance of the model using various pretrained word embeddings, with and without character embeddings, and with pretrained versus learned word embeddings",
      ". We evaluated our model on the 515 annotated history of present illness notes, which were split in a 70% train set, 15% development set, and a 15% test set. The model is trained using the Adam algorithm for gradient-based optimization BIBREF25 with an initial learning rate = 0.001 and decay = 0.9. A dropout rate of 0.5 was applied for regularization, and each batch size = 20. The model ran for 20 epochs and was halted early if there was no improvement after 3 epochs"
    ]
  },
  {
    "title": "ZuCo 2.0: A Dataset of Physiological Recordings During Natural Reading and Annotation",
    "answer": "The participants read 739 sentences in total, consisting of 349 normal reading sentences and 390 task-specific reading sentences.",
    "evidence": [
      ". The sentences were chosen in the same length range as ZuCo 1.0, and with similar Flesch reading ease scores. The dataset statistics are shown in Table TABREF2. Of the 739 sentences, the participants read 349 sentences in a normal reading paradigm, and 390 sentences in a task-specific reading paradigm, in which they had to determine whether a certain relation type occurred in the sentence or not",
      ". Participants were told to read the sentences normally without any special instructions. Figure FIGREF8 (left) shows an example sentence as it was depicted on the screen during recording. As shown in Figure FIGREF8 (middle), the control condition for this task consisted of single-choice questions about the content of the previous sentence. 12% of randomly selected sentences were followed by such a comprehension question with three answer options on a separate screen.",
      ". Of the 739 sentences, the participants read 349 sentences in a normal reading paradigm, and 390 sentences in a task-specific reading paradigm, in which they had to determine whether a certain relation type occurred in the sentence or not. Table TABREF3 shows the distribution of the different relation types in the sentences of the task-specific annotation paradigm. Purposefully, there are 63 duplicates between the normal reading and the task-specific sentences (8% of all sentences)",
      ". The duration of the recording sessions was between 100 and 180 minutes, depending on the time required to set up and calibrate the devices, and the personal reading speed of the participants. 50 sentences, alternating between tasks: 50 sentences of normal reading, followed by 50 sentences of task-specific reading. The order of blocks and sentences within blocks was identical for all subjects. Each sentence block was preceded by a practice round of three sentences.",
      ". Thus, the task-specific annotation reading lead to shorter passes because the goal was merely to recognize a relation in the text, but not necessarily to process the every word in each sentence. This distinct reading behavior is shown in Figure FIGREF15, where fixations occur until the end of the sentence during normal reading, while during task-specific reading the fixations stop after the decisive words to detect a given relation type"
    ]
  },
  {
    "title": "Data Collection for Interactive Learning through the Dialog",
    "answer": "The data was collected using a crowdsourcing platform called CrowdFlower (CF), where CF workers interacted with a chat-like interface to help the system answer randomly selected questions from the Simple questions BIBREF7 dataset.",
    "evidence": [
      "Dataset Collection Process A perfect data collection scenario for our dataset would use real running dialog system providing general information from the knowledge base to real users. This system could then ask for explanations and answers for questions which it is not able to answer. However, getting access to systems with real users is usually hard. Therefore, we used the crowdsourcing platform CrowdFlower (CF) for our data collection",
      ". From this dataset we also took the correct answers in form of Freebase entities. Our dataset consists of standard data split into training, development and test files. The basic properties of those files are as follows: Each file contains complete dialogs enriched by outputs of NLU (see Section \"Natural Language Understanding (NLU)\" ) that were used during the data collection",
      ". Therefore, we can collect dialogs containing all information used for interactive learning and omit those parts that were not requested by the policy. We collected the dataset (see Section \"Dataset Collection Process\" ) that enables simulation where the policy can decide how much extra information to the question it requests. If the question is clear to the system it can attempt to answer the question without any other information",
      "...), – none of the above, interpreted as user is giving information requested by the system. Parsing of the dialog acts is made by hand written rules using templates and keyword spotting. The templates and keywords were manually collected from frequent expressions used by CF workers during preparation runs of the dataset collection process (google it, check wikipedia, I would need... $\\rightarrow $ Negate).",
      ". However, getting access to systems with real users is usually hard. Therefore, we used the crowdsourcing platform CrowdFlower (CF) for our data collection. A CF worker gets a task instructing them to use our chat-like interface to help the system with a question which is randomly selected from training examples of Simple questions BIBREF7 dataset"
    ]
  },
  {
    "title": "Distilling Translations with Visual Awareness",
    "answer": "This approach leads to the state of the art results on the Multi30K dataset.",
    "evidence": [
      ". This approach leads to the state of the art results. Additionally, we show that it has the ability to recover from erroneous or missing words in the source language.",
      "Data We build and test our MMT models on the Multi30K dataset BIBREF21 . Each image in Multi30K contains one English (EN) description taken from Flickr30K BIBREF22 and human translations into German (DE), French (FR) and Czech BIBREF23 , BIBREF24 , BIBREF25 . The dataset contains 29,000 instances for training, 1,014 for development, and 1,000 for test. We only experiment with German and French, which are languages for which we have in-house expertise for the type of analysis we present",
      "Results In this section we present results of our experiments, first in the original dataset without any source degradation (Section SECREF18 ) and then in the setup with various source degradation strategies (Section SECREF25 ).",
      ". In this setting (PERS), we use the Flickr Entities dataset BIBREF28 to identify all the words that were annotated by humans as corresponding to the category person. We then replace such source words with the BLANK token. The statistics of the resulting datasets for the three degradation strategies are shown in Table TABREF10 . We note that RND and PERS are the same for language pairs as the degradation only depends on the source side, while for AMB the words replaced depend on the target language.",
      ". In this setting (AMB), we rely on the MLT dataset BIBREF11 which provides a list of source words with multiple translations in the Multi30k training set. We replace ambiguous words with the BLANK token in the source language, which results in two language-specific datasets. In this setting (PERS), we use the Flickr Entities dataset BIBREF28 to identify all the words that were annotated by humans as corresponding to the category person. We then replace such source words with the BLANK token"
    ]
  },
  {
    "title": "English-Japanese Neural Machine Translation with Encoder-Decoder-Reconstructor",
    "answer": "They used two parallel corpora: Asian Scientific Paper Excerpt Corpus (ASPEC) and NTCIR PatentMT.",
    "evidence": [
      "Datasets We used two parallel corpora: Asian Scientific Paper Excerpt Corpus (ASPEC) BIBREF0 and NTCIR PatentMT Regarding the training data of ASPEC, we used only the first 1 million sentences sorted by sentence-alignment similarity. Japanese sentences were segmented by the morphological analyzer MeCab (version 0.996, IPADIC), and English sentences were tokenized by tokenizer.perl of Moses. Table TABREF14 shows the numbers of the sentences in each corpus",
      ". In order to confirm the language independence of the framework, we experiment on two parallel corpora of English-Japanese and Japanese-English translation tasks using encode-decoder-reconstructor. Our experiments show that their method offers significant improvement in BLEU scores and alleviates the problem of repeating and missing words in the translation on English-Japanese translation task, though the difference is not significant on Japanese-English translation task",
      ". Japanese sentences were segmented by the morphological analyzer MeCab (version 0.996, IPADIC), and English sentences were tokenized by tokenizer.perl of Moses. Table TABREF14 shows the numbers of the sentences in each corpus. Note that sentences with more than 40 words were excluded from the training data.",
      ". In their method, after training the forward translation in a manner similar to the conventional attention-based NMT, they train a back-translation model from the hidden state of the decoder into the source sequence by a new decoder to enforce agreement between source and target sentences. In order to confirm the language independence of the framework, we experiment on two parallel corpora of English-Japanese and Japanese-English translation tasks using encode-decoder-reconstructor",
      "Testing tu2016neural used a beam search to predict target sentences that approximately maximizes both of forward translation and back-translation on testing. In this paper, however, we do not use a beam search for simplicity and effectiveness."
    ]
  },
  {
    "title": "Stereotyping and Bias in the Flickr30K Dataset",
    "answer": "The paper provides a list of biases and unwarranted inferences that can be found in the Flickr30K dataset, but the specific biases are not explicitly stated in the given context.",
    "evidence": [
      ". I will build on earlier work on linguistic bias in general BIBREF3 , providing examples from the Flickr30K data, and present a taxonomy of unwarranted inferences. Finally, I will discuss several methods to analyze the data in order to detect biases.",
      "., 2013, p. 859). This paper presents some evidence against this assumption, and provides a list of biases and unwarranted inferences that can be found in the Flickr30K dataset. Finally, it considers methods to find examples of these, and discusses how we should deal with stereotype-driven descriptions in future applications.",
      ". How can we tell whether or not the data is actually biased? We don't know whether or not an entity belongs to a particular social class (in this case: ethnic group) until it is marked as such. But we can approximate the proportion by looking at all the images where the annotators have used a marker (in this case: adjectives like black, white, asian), and for those images count how many descriptions (out of five) contain a marker",
      "Discussion In the previous section, I have outlined several methods to manually detect stereotypes, biases, and odd phrases. Because there are many ways in which a phrase can be biased, it is difficult to automatically detect bias from the data. So how should we deal with stereotype-driven descriptions?",
      ". And so language models trained on this data may propagate harmful stereotypes, such as the idea that women are less suited for leadership positions. This paper aims to give an overview of linguistic bias and unwarranted inferences resulting from stereotypes and prejudices. I will build on earlier work on linguistic bias in general BIBREF3 , providing examples from the Flickr30K data, and present a taxonomy of unwarranted inferences"
    ]
  },
  {
    "title": "An Evaluation Dataset for Intent Classification and Out-of-Scope Prediction",
    "answer": "The dataset contains 23,700 queries, including 22,500 in-scope queries and 1,200 out-of-scope queries.",
    "evidence": [
      "In addition to the full dataset, we consider three variations. First, Small, in which there are only 50 training queries per each in-scope intent, rather than 100. Second, Imbalanced, in which intents have either 25, 50, 75, or 100 training queries. Third, OOS+, in which there are 250 out-of-scope training examples, rather than 100. These are intended to represent production scenarios where data may be in limited or uneven supply.",
      "Dataset We introduce a new crowdsourced dataset of 23,700 queries, including 22,500 in-scope queries covering 150 intents, which can be grouped into 10 general domains. The dataset also includes 1,200 out-of-scope queries. Table TABREF2 shows examples of the data.",
      ". However, the Snips dataset has only a small number of intents and an enormous number of examples of each. Snips does present a low-data variation, with 70 training queries per intent, in which performance drops slightly. The dataset presented in BIBREF10 has a large number of intent classes, yet also contains a wide range of samples per intent class (ranging from 24 to 5,981 queries per intent, and so is not constrained in all cases)",
      "Benchmark Evaluation To quantify the challenges that our new dataset presents, we evaluated the performance of a range of classifier models and out-of-scope prediction schemes.",
      ". The dataset presented in BIBREF10 has a large number of intent classes, yet also contains a wide range of samples per intent class (ranging from 24 to 5,981 queries per intent, and so is not constrained in all cases). created datasets with constrained training data, but with very few intents, presenting a very different type of challenge. We also include the TREC query classification datasets BIBREF7, which have a large set of labels, but they describe the desired response type (e.g"
    ]
  },
  {
    "title": "Automatic Judgment Prediction via Legal Reading Comprehension",
    "answer": "The dataset used in the experiment is constructed by randomly collecting cases from China Judgments Online, with 1 case for training, 2 cases for validation and testing each, and it includes 3 granted divorce cases and others not granted.",
    "evidence": [
      "Dataset Construction for Evaluation Since none of the datasets from previous works have been published, we decide to build a new one. We randomly collect INLINEFORM0 cases from China Judgments Online, among which INLINEFORM1 cases are for training, INLINEFORM2 each for validation and testing. Among the original cases, INLINEFORM3 are granted divorce and others not. There are INLINEFORM4 valid pleas in total, with INLINEFORM5 supported and INLINEFORM6 rejected",
      "Baselines For comparison, we adopt and re-implement three kinds of baselines as follows: We implement an SVM with lexical features in accordance with previous works BIBREF16 , BIBREF17 , BIBREF1 , BIBREF15 , BIBREF4 and select the best feature set on the development set. We implement and fine-tune a series of neural text classifiers, including attention-based method BIBREF3 and other methods we deem important",
      ". (3) Besides baselines from previous works, we also carry out comprehensive experiments comparing different existing deep neural network methods on our dataset. Supported by these experiments, improvements achieved by LRC prove to be robust.",
      ". Experimental results show that our model achieves considerable improvement than all the baselines. Besides, visualization results also demonstrate the effectiveness and interpretability of our proposed model. In the future, we can explore the following directions: (1) Limited by the datasets, we can only verify our proposed model on divorce proceedings. A more general and larger dataset will benefit the research on judgment prediction",
      ". The experiments support our hypothesis as proposed in the Introduction part that in civil cases, it's important to model the interactions among case materials. Reading mechanism can well perform the matching among them."
    ]
  },
  {
    "title": "Improved Neural Relation Detection for Knowledge Base Question Answering",
    "answer": "The core component for KBQA is the KB query generation, which retrieves answers from a Knowledge Base.",
    "evidence": [
      "Introduction Knowledge Base Question Answering (KBQA) systems answer questions by obtaining information from KB tuples BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3 , BIBREF4 , BIBREF5 . For an input question, these systems typically generate a KB query, which can be executed to retrieve the answers from a KB",
      "KBQA Enhanced by Relation Detection This section describes our KBQA pipeline system. We make minimal efforts beyond the training of the relation detection model, making the whole system easy to build. Following previous work BIBREF4 , BIBREF5 , our KBQA system takes an existing entity linker to produce the top- $K$ linked entities, $EL_K(q)$ , for a question $q$ (“initial entity linking”)",
      ". For an input question, these systems typically generate a KB query, which can be executed to retrieve the answers from a KB. Figure 1 illustrates the process used to parse two sample questions in a KBQA system: (a) a single-relation question, which can be answered with a single $<$ head-entity, relation, tail-entity $>$ KB tuple BIBREF6 , BIBREF7 , BIBREF2 ; and (b) a more complex case, where some constraints need to be handled for multiple entities in the question",
      ". Note that in contrast to previous KBQA systems, our system does not use joint-inference or feature-based re-ranking step, nevertheless it still achieves better or comparable results to the state-of-the-art. The third block of the table details two ablation tests for the proposed components in our KBQA systems: (1) Removing the entity re-ranking step significantly decreases the scores",
      ". KBQA with two-step relation detection  Compared to previous approaches, the main difference is that we have an additional entity re-ranking step after the initial entity linking. We have this step because we have observed that entity linking sometimes becomes a bottleneck in KBQA systems. For example, on SimpleQuestions the best reported linker could only get 72.7% top-1 accuracy on identifying topic entities. This is usually due to the ambiguities of entity names, e.g"
    ]
  },
  {
    "title": "Knowledge Graph Representation with Jointly Structural and Textual Encoding",
    "answer": "The paper uses WN18 (a subset of WordNet) and FB15K (a subset of Freebase) to evaluate its proposed models.",
    "evidence": [
      "Datasets We use two popular knowledge bases: WordNet BIBREF0 and Freebase BIBREF1 in this paper. Specifically, we use WN18 (a subset of WordNet) BIBREF24 and FB15K (a subset of Freebase) BIBREF2 since their text descriptions are easily publicly available. Table 1 lists statistics of the two datasets.",
      ". We call this evaluation setting “Filter”. The evaluation results are reported under these two settings. We select the margin $\\gamma $ among $\\lbrace 1, 2\\rbrace $ , the embedding dimension $d$ among $\\lbrace 20, 50, 100\\rbrace $ , the regularization $\\eta $ among $\\lbrace 0, 1E{-5}, 1E{-6}\\rbrace $ , two learning rates $\\lambda _s$ and $\\lambda _t$ among $\\lbrace 0.001, 0.01, 0.05\\rbrace $ to learn the parameters of structure and text encoding",
      "Experiment In this section, we study the empirical performance of our proposed models on two benchmark tasks: triplet classification and link prediction.",
      ". Here, we set $p=10$ for entities and $p=1$ for relations. A lower Mean Rank and a higher Hits@p should be achieved by a good embedding model. Since a false predicted triplet may also exist in knowledge graphs, it should be regard as a valid triplet. Hence, we should remove the false predicted triplets included in training, validation and test sets before ranking (except the test triplet of interest). We call this evaluation setting “Filter”. The evaluation results are reported under these two settings",
      ". The difference among the existing methods varies between linear BIBREF2 , BIBREF8 and nonlinear BIBREF3 score functions in the low-dimensional vector space. Among these methods, TransE BIBREF2 is a simple and effective approach, which learns the vector embeddings for both entities and relationships"
    ]
  },
  {
    "title": "State-of-the-Art Vietnamese Word Segmentation",
    "answer": "Methods to model sentences and text in machine learning systems are briefly mentioned as one of the approaches applied to solve word segmentation in Vietnamese.",
    "evidence": [
      "CONCLUSIONS AND FUTURE WORKS This study reviewed state-of-the-art approaches and systems of Vietnamese word segmentation. The review pointed out common features and methods used in Vietnamese word segmentation studies. This study also had an evaluation of the existing Vietnamese word segmentation toolkits based on a same corpus to show advantages and disadvantages as to shed some lights on system enhancement. There are several challenges on supervised learning approaches in future work",
      ". In general, Chinese is a language which has the most studies on the word segmentation issue. However, there is a lack of survey of word segmentation studies on Asian languages and Vietnamese as well. This paper aims reviewing state-of-the-art word segmentation approaches and systems applying for Vietnamese. This study will be a foundation for studies on Vietnamese word segmentation and other following Vietnamese tasks as well, such as part-of-speech tagger, chunker, or parser systems",
      ". A all sided review of word segmentation will help next studies on Vietnamese natural language processing tasks have an up-to-date guideline and choose the most suitable solution for the task. The remaining part of the paper is organized as follows. Section II discusses building corpus in Vietnamese, containing linguistic issues and the building progress. Section III briefly mentions methods to model sentences and text in machine learning systems",
      ". According to our observation, we found that is lacks of complete review approaches, datasets and toolkits which we recently used in Vietnamese word segmentation. A all sided review of word segmentation will help next studies on Vietnamese natural language processing tasks have an up-to-date guideline and choose the most suitable solution for the task. The remaining part of the paper is organized as follows",
      "Abstract Word segmentation is the first step of any tasks in Vietnamese language processing. This paper reviews stateof-the-art approaches and systems for word segmentation in Vietnamese. To have an overview of all stages from building corpora to developing toolkits, we discuss building the corpus stage, approaches applied to solve the word segmentation and existing toolkits to segment words in Vietnamese sentences"
    ]
  },
  {
    "title": "A Multi-Task Architecture on Relevance-based Neural Query Translation",
    "answer": "The baselines are reported in Table TABREF19, comparing the average precision and recall for both the transformer model and the proposed model across the train and validation query set over two language pairs.",
    "evidence": [
      ". This objective is similar to CBOW word embedding as context is used to predict pivot word Here, we use a scaling factor INLINEFORM6 , to have a balance between the gradients from the NMT loss and RAT loss. For RAT, the context is drawn from a context window following BIBREF6 . In the figure, INLINEFORM0 and INLINEFORM1 represents the top document retrieved against INLINEFORM2 . The shuffler component shuffles INLINEFORM3 and INLINEFORM4 and creates (context, pivot) pairs",
      ". Given a query INLINEFORM0 , consider INLINEFORM1 as the set of terms from human translation of INLINEFORM2 and INLINEFORM3 as the set of translation terms generated by model INLINEFORM4 . We define INLINEFORM5 and INLINEFORM6 as precision and recall of INLINEFORM7 for model INLINEFORM8 . In Table TABREF19 , we report average precision and recall for both transformer and our model across our train and validation query set over two language pairs. Our model generates precise translation, i.e",
      ". We select the single top document retrieved against a sentence INLINEFORM0 because a sentence is a weak representation of information need. As a result, documents at lower ranks show heavy shift from the context of the sentence query. We verified this by observing that a relevance model constructed from the top INLINEFORM1 documents does not perform well in this setting",
      ". The second one is the retrieval corpus, which is a collection of INLINEFORM1 documents INLINEFORM2 in the same language as INLINEFORM3 . Our word-embedding approach takes each INLINEFORM4 , uses it as a query to retrieve the top document INLINEFORM5 . After that we obtain INLINEFORM6 by concatenating INLINEFORM7 with INLINEFORM8 and randomly shuffling the words in the combined sequence. We then augment INLINEFORM9 using INLINEFORM10 and obtain a dataset, INLINEFORM11",
      ". We also use a similar loss function to train word embedding over a set of context ( INLINEFORM1 ) and pivot ( INLINEFORM2 ) pairs formed using INLINEFORM3 as query to retrieve INLINEFORM4 using Query Likelihood (QL) ranker, INLINEFORM5 . This objective is similar to CBOW word embedding as context is used to predict pivot word Here, we use a scaling factor INLINEFORM6 , to have a balance between the gradients from the NMT loss and RAT loss"
    ]
  },
  {
    "title": "Unsupervised Learning of Syntactic Structure with Invertible Neural Projections",
    "answer": "The invertibility condition is that the neural projector is constrained by two requirements: (1) INLINEFORM0 exists, and (2) INLINEFORM1 exists.",
    "evidence": [
      "Learning & Inference In this section, we introduce an invertibility condition for our neural projector to tackle the optimization challenge. Specifically, we constrain our neural projector with two requirements: (1) INLINEFORM0 and (2) INLINEFORM1 exists. Invertible transformations have been explored before in independent components analysis BIBREF14 , gaussianization BIBREF15 , and deep density models BIBREF16 , BIBREF17 , BIBREF18 , for unstructured data",
      ". To address this issue, several recent papers propose specially designed invertible networks that are easily trainable yet still powerful BIBREF16 , BIBREF17 , BIBREF19 . Inspired by these works, we use the invertible transformation proposed by BIBREF16 , which consists of a series of “coupling layers”. This architecture is specially designed to guarantee a unit Jacobian determinant (and thus the invertibility property)",
      ". ( EQREF14 ) and obtain : INLINEFORM3  By using the change of variable rule to the integration, which allows the integration variable INLINEFORM0 to be replaced by INLINEFORM1 , the marginal emission factor can be computed in closed-form when the invertibility condition is satisfied: DISPLAYFORM0  where INLINEFORM0 is a conditional Gaussian distribution, INLINEFORM1 is the Jacobian matrix of function INLINEFORM2 at INLINEFORM3 , and INLINEFORM4 represents the absolute value of its determinant",
      ". ( EQREF17 ), which means inference and learning may be difficult. In sec:opt, we address this issue by constraining INLINEFORM3 to be invertible, and show that this constraint enables tractable exact inference and marginal likelihood computation.",
      ". Under the invertibility condition, we derive a learning algorithm and give another view of our approach revealed by the objective function. Then, we present the architecture of a neural projector we use in experiments: a volume-preserving invertible neural network proposed by BIBREF16 for independent components estimation."
    ]
  },
  {
    "title": "Crowdsourcing for Beyond Polarity Sentiment Analysis A Pure Emotion Lexicon",
    "answer": "They compare crowd evaluations to those of expert linguists to determine the quality of the lexicon.",
    "evidence": [
      ". Participants submit their answers that go through a filtering process. If the answers are considered valid, they update the lexicon entries. The crowd also evaluates existing annotations, to determine the lexicon quality. As crowd evaluation methods are new in lexicon acquisition tasks, we compare crowd evaluations to those of expert linguists.",
      "Lexicon Analysis The lexicon (will be referred as simply \"PEL\") is created after the exclusion of annotations following the 40% single annotation filtering check. We received more than seventy thousands annotations for 10593 term groups, of those only 22 thousand annotations for 9737 term groups are included in the final lexicon, as a result of filtering. Each term group had a mean 2.3 annotations from a total of 95 different annotators",
      ". Our goal is to maintain an end to end automated work-flow for a crowdsourced (annotation and evaluation wise) lexicon acquisition process. Therefore, to highlight crowd's performance on evaluation, we compare evaluations from linguistic experts and the crowd itself.",
      ". Term groups did not create confusion amongst workers, and only a small fraction of term groups had subclass agreement. On the contrary, including the stem and description in the task confused workers, and were excluded from the interface. We tested several interface designs, and the one that worked best had minimal instructions. Lexicon acquisition interfaces in paid micro-task environments should be further studied, with regards to various other contribution incentives",
      ". There was a number of term groups with opposite emotion dyads, presented in Table TABREF33 ,but as the number of annotations increases, emotional agreement occurrences -combination or opposition- decreases. In total, the lexicon features 17740 annotated terms with 3 classes and 11 subclasses. The dominant class for 7030 terms was emotion, 191 intensifying, 6801 none, and 3718 in some form of subclass agreement"
    ]
  },
  {
    "title": "Explicit Utilization of General Knowledge in Machine Reading Comprehension",
    "answer": "KAR is a type of Machine Reading Comprehension (MRC) model.",
    "evidence": [
      "Analysis According to the experimental results, KAR is not only comparable in performance with the state-of-the-art MRC models, but also superior to them in terms of both the hunger for data and the robustness to noise. The reasons for these achievements, we believe, are as follows:",
      "Model Comparison in both Performance and the Robustness to Noise We compare KAR with other MRC models in both performance and the robustness to noise. Specifically, we not only evaluate the performance of KAR on the development set and the test set, but also do this on the adversarial sets. As for the comparative objects, we only consider the single MRC models that rank in the top 20 on the SQuAD 1.1 leader board and have reported their performance on the adversarial sets",
      ". Based on the data enrichment method, KAR is comparable in performance with the state-of-the-art MRC models, and significantly more robust to noise than them. When only a subset ( INLINEFORM0 – INLINEFORM1 ) of the training examples are available, KAR outperforms the state-of-the-art MRC models by a large margin, and is still reasonably robust to noise.",
      ". As shown in Table TABREF12 , on the development set and the test set, the performance of KAR is on par with that of the state-of-the-art MRC models; on the adversarial sets, KAR outperforms the state-of-the-art MRC models by a large margin. That is to say, KAR is comparable in performance with the state-of-the-art MRC models, and significantly more robust to noise than them",
      "Model Comparison in the Hunger for Data We compare KAR with other MRC models in the hunger for data. Specifically, instead of using all the training examples, we produce several training subsets (i.e. subsets of the training examples) so as to study the relationship between the proportion of the available training examples and the performance. We produce each training subset by sampling a specific number of questions from all the questions relevant to each passage"
    ]
  },
  {
    "title": "Imparting Interpretability to Word Embeddings while Preserving Semantic Structure",
    "answer": "The semantically related words take larger values along a specified dimension that corresponds to the word-group under consideration.",
    "evidence": [
      ". This again illustrates that a semantic concept can indeed be coded to a vector dimension provided that a sensible lexical resource is used to guide semantically related words to the desired vector dimension via the proposed objective function in ( SECREF4 ). Even the words that do not appear in, but are semantically related to, the word-groups that we formed using Roget's Thesaurus, are indirectly affected by the proposed algorithm",
      ". Neither the vector nor its dimensions have any absolute, interpretable meaning. We introduce an additive modification to the objective function of the embedding learning algorithm that encourages the embedding vectors of words that are semantically related to a predefined concept to take larger values along a specified dimension, while leaving the original semantic learning mechanism mostly unaffected. In other words, we align words that are already determined to be related, along predefined concepts",
      ". Specifically, the vector representations of words that belong to a particular group are encouraged to have deliberately increased values in a particular dimension that corresponds to the word-group under consideration. This can be achieved by modifying the objective function of the embedding algorithm to partially influence vector representation distributions across their dimensions over an input vocabulary",
      ". Each embedding vector dimension is first associated with a concept. For a word belonging to any one of the word-groups representing these concepts, the modified cost term favors an increase for the value of this word's embedding vector dimension corresponding to the concept that the particular word belongs to. For words that do not belong to any one of the word-groups, the cost term is left untouched",
      ". The simultaneous minimization of both objectives ensures that words that are similar to, but not included in, one of these concept word-groups are also “nudged” towards the associated dimension INLINEFORM7 . The trained word vectors are thus encouraged to form a distribution where the individual vector dimensions align with certain semantic concepts represented by a collection of concept word-groups, one assigned to each vector dimension"
    ]
  },
  {
    "title": "Stance Detection in Turkish Tweets",
    "answer": "Galatasaray and Fenerbahçe are the targets.",
    "evidence": [
      ". Hence we have determined our targets as Galatasaray (namely Target-1) and Fenerbahçe (namely, Target-2) which are two of the most popular football clubs in Turkey. As is the case for the sentiment analysis tools, the outputs of the stance detection systems on a stream of tweets about these clubs can facilitate the use of the opinions of the football followers by these clubs",
      ". We have decided to use one of these sets (corresponding to the period between August 18 and September 6, 2015) and firstly filtered the tweets using the possible names used to refer to the target clubs. Then, we have annotated the stance information in the tweets for these targets as Favor or Against. Within the course of this study, we have not considered those tweets in which the target is not explicitly mentioned, as our initial filtering process reveals",
      "A Stance Detection Data Set We have decided to consider tweets about popular sports clubs as our domain for stance detection. Considerable amounts of tweets are being published for sports-related events at every instant. Hence we have determined our targets as Galatasaray (namely Target-1) and Fenerbahçe (namely, Target-2) which are two of the most popular football clubs in Turkey",
      ". The performance of the classifiers is better for the Favor class for both targets when compared with the performance results for the Against class. This outcome may be due to the common use of some terms when expressing positive stance towards sports clubs in Turkish tweets. The same percentage of common terms may not have been observed in tweets during the expression of negative stances towards the targets",
      ". For the purposes of the current study, we have not annotated any tweets with the Neither class. This stance class and even finer-grained classes can be considered in further annotation studies. We should also note that in a few tweets, the target of the stance was the management of the club while in some others a particular footballer of the club is praised or criticised. Still, we have considered the club as the target of the stance in all of the cases and carried out our annotations accordingly"
    ]
  },
  {
    "title": "Deep Learning the EEG Manifold for Phonological Categorization from Active Thoughts",
    "answer": "The EEG data comes from 14 participants.",
    "evidence": [
      "Joint variability of electrodes Multichannel EEG data is high dimensional multivariate time series data whose dimensionality depends on the number of electrodes. It is a major hurdle to optimally encode information from these EEG data into lower dimensional space. In fact, our investigation based on a development set (as we explain later) showed that well-known deep neural networks (e.g",
      "Training and hyperparameter selection We performed two sets of experiments with the single-trial EEG data. In PHASE-ONE, our goals was to identify the best architectures and hyperparameters for our networks with a reasonable number of runs. For PHASE-ONE, we randomly shuffled and divided the data (1913 signals from 14 individuals) into train (80%), development (10%) and test sets (10%)",
      ". This is essentially different than our previous work BIBREF16 where we extract per-channel 1-D covariance information and feed it to the networks. We present our sample 2-D EEG cross-covariance matrices (of two individuals) in Fig.",
      ". Besides, we found that instead of using the raw multi-channel high-dimensional EEG requiring large training times and resource requirements, it is advantageous to first reduce its dimensionality by capturing the information transfer among the electrodes. Instead of the conventional approach of selecting a handful of channels as BIBREF17 , BIBREF18 , we address this by computing the channel cross-covariance, resulting in positive, semi-definite matrices encoding the connectivity of the electrodes",
      ". The dataset consists of 14 participants, with each prompt presented 11 times to each individual. Since our intention is to classify the phonological categories from human thoughts, we discard the facial and audio information and only consider the EEG data corresponding to imagined speech. It is noteworthy that given the mixed nature of EEG signals, it is reportedly challenging to attain a pairwise EEG-phoneme mapping BIBREF18"
    ]
  },
  {
    "title": "Towards Understanding Neural Machine Translation with Word Importance",
    "answer": "They test their word importance approach on NMT (Neural Machine Translation) models.",
    "evidence": [
      "Analysis In this section, we conduct analyses on two potential usages of word importance, which can help debug NMT models (Section SECREF33) and design better architectures for specific languages (Section SECREF37). Due to the space limitation, we only analyze the results of Chinese$\\Rightarrow $English, English$\\Rightarrow $French, and English$\\Rightarrow $Japanese. We list the results on the reverse directions in Appendix, in which the general conclusions also hold.",
      ". All the following experiments are conducted on the test dataset, and we estimate the input word importance using the model generated hypotheses. In the following experiments, we compare IG (Attribution) with several black-box methods (i.e., Content, Frequency, Attention) as introduced in Section SECREF8",
      ". We categorize the methods of word importance estimation into two types: black-box methods without the knowledge of the model and white-box methods that have access to the model internal information (e.g., parameters and gradients). Previous studies mostly fall into the former type, and in this study, we investigate several representative black-box methods: Content Words: In linguistics, all words can be categorized as either content or content-free words",
      "Introduction ::: Contributions Our main contributions are: Our study demonstrates the necessity and effectiveness of exploiting the intermediate gradients for estimating word importance. We find that word importance is useful for understanding NMT by identifying under-translated words. We provide empirical support for the design principle of NMT architectures: essential inductive bias (e.g., language characteristics) should be considered for model design.",
      "In this work, the notion of “word importance” is employed to quantify the contribution that a word in the input sentence makes to the NMT generations. We categorize the methods of word importance estimation into two types: black-box methods without the knowledge of the model and white-box methods that have access to the model internal information (e.g., parameters and gradients)"
    ]
  },
  {
    "title": "How Contextual are Contextualized Word Representations? Comparing the Geometry of BERT, ELMo, and GPT-2 Embeddings",
    "answer": "They implicitly factorize a word-context matrix containing a co-occurrence statistic.",
    "evidence": [
      ". Though they learn embeddings iteratively in practice, it has been proven that in theory, they both implicitly factorize a word-context matrix containing a co-occurrence statistic BIBREF7, BIBREF8. Because they create a single representation for each word, a notable problem with static word embeddings is that all senses of a polysemous word must share a single vector.",
      ". It gives us an upper bound on how well a static embedding could replace a word's contextualized representations. The closer $\\textit {MEV}_\\ell (w)$ is to 0, the poorer a replacement a static embedding would be; if $\\textit {MEV}_\\ell (w) = 1$, then a static embedding would be a perfect replacement for the contextualized representations.",
      ". We ultimately found that after adjusting for anisotropy, on average, less than 5% of the variance in a word's contextualized representations could be explained by a static embedding. This means that even in the best-case scenario, in all layers of all models, static word embeddings would be a poor replacement for contextualized ones. These insights help explain some of the remarkable success that contextualized representations have had on a diverse array of NLP tasks.",
      ". Traditionally, these word embeddings were static: each word had a single vector, regardless of context BIBREF0, BIBREF1. This posed several problems, most notably that all senses of a polysemous word had to share the same representation. More recent work, namely deep neural language models such as ELMo BIBREF2 and BERT BIBREF3, have successfully created contextualized word representations, word vectors that are sensitive to the context in which they appear",
      ". Another direction for future work is generating static word representations from contextualized ones. While the latter offer superior performance, there are often challenges to deploying large models such as BERT in production, both with respect to memory and run-time. In contrast, static representations are much easier to deploy. Our work in section 4"
    ]
  },
  {
    "title": "Paraphrase-Supervised Models of Compositionality",
    "answer": "They compare against a hierarchical phrase-based system with a 4-gram language model, with feature weights tuned using MIRA.",
    "evidence": [
      ". Thus, if we reveal to the translation system that a phrase is non-compositional, it should be able to learn that translation decisions which translate it as a unit are to be favored, leading to better translations. To test this hypothesis, we built an English-Spanish MT system using the cdec decoder BIBREF27 for the entire training pipeline (word alignments, phrase extraction, feature weight tuning, and decoding)",
      ". To this end, we consider the problem of machine translation, operating under the hypothesis that sentences which express their meaning non-compositionally should also translate non-compositionally. Modern phrase-based translation systems are faced with a large number of possible segmentations of a source-language sentence during decoding, and all segmentations are considered equally likely BIBREF13",
      ". Corpora from the WMT 2011 evaluation was used to build the translation and language models, and for tuning (on news-test2010) and evaluation (on news-test2011), with scoring done using BLEU BIBREF28 . The baseline is a hierarchical phrase-based system BIBREF29 with a 4-gram language model, with feature weights tuned using MIRA BIBREF30",
      ". However, the primary aim of their work is phrase extraction for MT, and the non-compositional constraints are only applied to make the space of phrase pairs more tractable when bootstrapping their phrasal parser from their word-based parser. In contrast, we score every phrase that is extracted with the standard phrase extraction heuristics BIBREF29 , allowing the decoder to make the final decision on the impact of compositionality scores in translation",
      ". And third, an evaluation of the impact of compositionality knowledge in an end-to-end MT setup. Our experiments (§ SECREF5 ) reveal that using supervision from automatically extracted paraphrases produces compositional functions with equivalent performance to previous approaches that have relied on hand-annotated training data. Furthermore, compositionality features consistently improve the translations produced by a strong English–Spanish translation system."
    ]
  },
  {
    "title": "Incorporating Subword Information into Matrix Factorization Word Embeddings",
    "answer": "Our model performs explicit matrix factorization using stochastic gradient descent.",
    "evidence": [
      ". Closely related are models that use morphological segmentation in learning word representations BIBREF20 , BIBREF21 , BIBREF22 , BIBREF23 , BIBREF24 , BIBREF25 . Our model also uses n-grams and morphological segmentation, but it performs explicit matrix factorization to learn subword and word representations, unlike these related models which mostly use neural networks. Finally, BIBREF26 and BIBREF27 retrofit morphological information onto pre-trained models",
      ". These embeddings are usually derived by employing the distributional hypothesis: that similar words appear in similar contexts BIBREF0 . The models that perform the word embedding can be divided into two classes: predictive, which learn a target or context word distribution, and counting, which use a raw, weighted, or factored word-context co-occurrence matrix BIBREF1 . The most well-known predictive model, which has become eponymous with word embedding, is word2vec BIBREF2",
      "Subword LexVec The LexVec BIBREF7 model factorizes the PPMI-weighted word-context co-occurrence matrix using stochastic gradient descent. $$PPMI_{wc} = max(0, \\log \\frac{M_{wc} \\; M_{**}}{ M_{w*} \\; M_{*c} })$$   (Eq. 3)  where $M$ is the word-context co-occurrence matrix constructed by sliding a window of fixed size centered over every target word  $w$ in the subsampled BIBREF2 training corpus and incrementing cell $M_{wc}$ for every context word $c$ appearing within this window (forming a $(w,c)$ pair)",
      "Related Work Word embeddings that leverage subword information were first introduced by BIBREF14 which represented a word of as the sum of four-gram vectors obtained running an SVD of a four-gram to four-gram co-occurrence matrix. Our model differs by learning the subword vectors and resulting representation jointly as weighted factorization of a word-context co-occurrence matrix is performed",
      ". LexVec adjusts the PPMI matrix using context distribution smoothing BIBREF3 . With the PPMI matrix calculated, the sliding window process is repeated and the following loss functions are minimized for every observed $(w,c)$ pair and target word $w$ :  $$L_{wc} &= \\frac{1}{2} (u_w^\\top v_c - PPMI_{wc})^2 \\\\  L_{w} &= \\frac{1}{2} \\sum \\limits _{i=1}^k{\\mathbf {E}_{c_i \\sim P_n(c)} (u_w^\\top v_{c_i} - PPMI_{wc_i})^2 }$$   (Eq. 4)   where $u_w$ and $v_c$ are $d$ -dimensional word and context vectors"
    ]
  },
  {
    "title": "Abstractive Summarization for Low Resource Data using Domain Transfer and Data Synthesis",
    "answer": "The recent abstractive summarization methods explored in this paper include state-of-the-art models trained on newspaper data and a template-based synthesis method.",
    "evidence": [
      ". Second, we propose a template-based synthesis method to create synthesized summaries, then explore the effect of enriching training data for abstractive summarization using the proposed model compared to a synthesis baseline. Evaluations of neural abstractive summarization method across four student reflection corpora show the utility of all three methods.",
      ". In a different approach to abstractive summarization, BIBREF11 developed a soft template based neural method consisting of an end-to-end deep model for template retrieval, reranking and summary rewriting. While we also develop a template based model, our work differs in both model structure and purpose. Data synthesis for text summarization is underexplored, with most prior work focusing on machine translation, and text normalization",
      "Introduction Recently, with the emergence of neural seq2seq models, abstractive summarization methods have seen great performance strides BIBREF0, BIBREF1, BIBREF2. However, complex neural summarization models with thousands of parameters usually require a large amount of training data. In fact, much of the neural summarization work has been trained and tested in news domains where numerous large datasets exist",
      "Abstract Training abstractive summarization models typically requires large amounts of data, which can be a limitation for many domains. In this paper we explore using domain transfer and data synthesis to improve the performance of recent abstractive summarization methods when applied to small corpora of student reflections. First, we explored whether tuning state of the art model trained on newspaper data could boost performance on student reflection data",
      "Abstractive summarization aims to generate coherent summaries with high readability, and has seen increasing interest and improved performance due to the emergence of seq2seq models BIBREF8 and attention mechanisms BIBREF9. For example, BIBREF0, BIBREF2, and BIBREF1 in addition to using encoder-decoder model with attention, they used pointer networks to solve the out of vocabulary issue, while BIBREF0 used coverage mechanism to solve the problem of word repetition"
    ]
  },
  {
    "title": "Revisiting Summarization Evaluation for Scientific Articles",
    "answer": "The common belief that this paper refutes is that ROUGE has a high correlation with human assessment scores in various summarization tasks.",
    "evidence": [
      ". However, later research has casted doubts about the accuracy of Rouge against manual evaluations. conroy2008mind analyzed DUC 2005 to 2007 data and showed that while some systems achieve high Rouge scores with respect to human summaries, the linguistic and responsiveness scores of those systems do not correspond to the high Rouge scores. We studied the effectiveness of Rouge through correlation analysis with manual scores",
      ". Since 2003, summarization has grown to much further domains and genres such as scientific documents, social media and question answering. While there is not enough compelling evidence about the effectiveness of Rouge on these other summarization tasks, published research is almost always evaluated by Rouge. In addition, Rouge has a large number of possible variants and the published research often (arbitrarily) reports only a few of these variants",
      ". Results reveal that, contrary to the common belief, correlations between Rouge and the Pyramid scores are weak, which challenges its effectiveness for scientific summarization. Furthermore, we show a large variance of correlations between different Rouge variants and the manual evaluations which further makes the reliability of Rouge for evaluating scientific summaries less clear",
      ". In addition, Rouge has a large number of possible variants and the published research often (arbitrarily) reports only a few of these variants. [2]Document Understanding Conference (DUC) was one of NIST workshops that provided infrastructure for evaluation of text summarization methodologies (http://duc.nist.gov/). By definition, Rouge solely relies on lexical overlaps (such as n-gram and sequence overlaps) between the system generated and human written gold-standard summaries",
      "Related work Rouge BIBREF1 assesses the content quality of a candidate summary with respect to a set of human gold summaries based on their lexical overlaps. Since its introduction, Rouge has been one of the most widely reported metrics in the summarization literature, and its high adoption has been due to its high correlation with human assessment scores in DUC datasets BIBREF1 . However, later research has casted doubts about the accuracy of Rouge against manual evaluations"
    ]
  },
  {
    "title": "Edinburgh Neural Machine Translation Systems for WMT 16",
    "answer": "Our baseline systems are attentional encoder-decoder networks based on the dl4mt-tutorial.",
    "evidence": [
      "Baseline System Our systems are attentional encoder-decoder networks BIBREF0 . We base our implementation on the dl4mt-tutorial, which we enhanced with new features such as ensemble decoding and pervasive dropout. We use minibatches of size 80, a maximum sentence length of 50, word embeddings of size 500, and hidden layers of size 1024. We clip the gradient norm to 1.0 BIBREF4 . We train the models with Adadelta BIBREF5 , reshuffling the training corpus between epochs",
      ". All reported methods give substantial improvements, and we see improvements of 4.3--11.2 BLEU over our baseline systems. In the human evaluation, our systems were the (tied) best constrained system for 7 out of 8 translation directions in which we participated.",
      ".5M sentences of back-translated data, 5 copies of news-commentary v11, and a matching quantity of data sampled from Czeng 1.6pre. After training this to convergence, we restarted training from the baseline model using 5M sentences of back-translated data, 5 copies of news-commentary v11, and a matching quantity of data sampled from Czeng 1.6pre. We repeated this with 7.5M sentences from news2015 monolingual, and then with 10M sentences of news2015",
      ". Pervasive dropout on all layers was used for English INLINEFORM0 Romanian, and gave substantial improvements. For English INLINEFORM1 German and English INLINEFORM2 Czech, we trained a right-to-left model with reversed target side, and we found reranking the system output with these reversed models helpful.",
      "Shared Task Results Table TABREF22 shows the ranking of our submitted systems at the WMT16 shared news translation task. Our submissions are ranked (tied) first for 5 out of 8 translation directions in which we participated: EN INLINEFORM0 CS, EN INLINEFORM1 DE, and EN INLINEFORM2 RO. They are also the (tied) best constrained system for EN INLINEFORM3 RU and RO INLINEFORM4 EN, or 7 out of 8 translation directions in total"
    ]
  },
  {
    "title": "Embracing data abundance: BookTest Dataset for Reading Comprehension",
    "answer": "They show that a majority of questions that our system could not answer so far are in fact answerable, suggesting that the original human baselines might have been underestimated.",
    "evidence": [
      ". This suggests that with current techniques there is only limited room for improvement on the algorithmic side. The other possibility to improve performance is simply to use more training data. The importance of training data was highlighted by the frequently quoted Mercer's statement that “There is no data like more data.” The observation that having more data is often more important than having better algorithms has been frequently stressed since then BIBREF13 , BIBREF14",
      "Baselines We will now use our psr model to evaluate the performance gain from increasing the dataset size.",
      "Possible Directions for Improvements Accuracy in any machine learning tasks can be enhanced either by improving a machine learning model or by using more in-domain training data. Current state of the art models BIBREF6 , BIBREF7 , BIBREF8 , BIBREF11 improve over AS Reader's accuracy on CBT NE and CN datasets by 1-2 percent absolute. This suggests that with current techniques there is only limited room for improvement on the algorithmic side",
      ". These have attracted a lot of attention from the research community BIBREF3 , BIBREF4 , BIBREF5 , BIBREF6 , BIBREF7 , BIBREF8 , BIBREF9 , BIBREF10 , BIBREF11 , BIBREF12 with a new state-of-the-art model coming out every few weeks. However if our goal is a production-level system actually capable of helping humans, we want the model to use all available resources as efficiently as possible",
      ". The results of the human study are summarized in Table TABREF28 . They show that a majority of questions that our system could not answer so far are in fact answerable. This suggests that 1) the original human baselines might have been underestimated, however, it might also be the case that there are some examples that can be answered by machines and not by humans; 2) there is still space for improvement"
    ]
  },
  {
    "title": "A Human-AI Loop Approach for Joint Keyword Discovery and Expectation Estimation in Micropost Event Detection",
    "answer": "The keyword-specific expectation is elicited from the crowd at each iteration after obtaining a new set of keyword-related microposts for which the model's prediction disagrees the most with the crowd's expectation.",
    "evidence": [
      ". More specifically, at each iteration after we obtain a keyword-specific expectation from the crowd, we train the model using expectation regularization and select those keyword-related microposts for which the model's prediction disagrees the most with the crowd's expectation; such microposts are then presented to the crowd to identify new keywords that best explain the disagreement",
      ". Our approach iteratively leverages the crowd to estimate both keyword specific expectation and the disagreement between the crowd and the model in order to discover new keywords that are most beneficial for model training. These keywords and their expectation not only improve the resulting performance but also make the model training process more transparent",
      ". As of now, we lack a principled method for discovering new keywords and improve the model performance. To address the above issues, we advocate a human-AI loop approach for discovering informative keywords and estimating their expectations reliably. Our approach iteratively leverages 1) crowd workers for estimating keyword-specific expectations, and 2) the disagreement between the model and the crowd for discovering new informative keywords",
      ". Our human-AI loop approach improves the inference of keyword expectation by aggregating contributions not only from the crowd but also from the model. This, however, comes with its own challenge as the model's predictions are further dependent on the results of expectation inference, which is used for model training",
      ". Such approaches are, however, limited as they fail to reliably estimate the informativeness of a keyword and its expectation for model training. This paper introduces a Human-AI loop approach to jointly discover informative keywords for model training while estimating their expectation. Our approach iteratively leverages the crowd to estimate both keyword specific expectation and the disagreement between the crowd and the model in order to discover new keywords that are most beneficial for model training"
    ]
  },
  {
    "title": "ARAML: A Stable Adversarial Training Framework for Text Generation",
    "answer": "The GAN models used as baselines to compare against are SeqGAN, LeakGAN, IRL, MaliGAN, and ARAML.",
    "evidence": [
      ". As for the details of the baselines, the generators of all the baselines except LeakGAN are the same as ours. Note that the generator of LeakGAN consists of a hierarchical LSTM unit, thus we followed the implementation in the original paper. In terms of the differences, the discriminators of GAN baselines are implemented based on the original papers",
      ". Note that the generator of LeakGAN consists of a hierarchical LSTM unit, thus we followed the implementation in the original paper. In terms of the differences, the discriminators of GAN baselines are implemented based on the original papers. Other hyper-parameters of baselines including batch size, learning rate, and pre-training epochs, were set based on the original codes, because the convergence of baselines is sensitive to these hyper-parameters.",
      "Further Analysis on Stability To verify the training stability, we conducted experiments on COCO many times and chose the best 5 trials for SeqGAN, LeakGAN, IRL, MaliGAN and ARAML, respectively. Then, we presented the forward/reverse perplexity in the training process in Figure FIGREF38 . We can see that our model with smaller standard deviation is more stable than other GAN baselines in both metrics",
      ". Other variants of GANs have been proposed to improve the generator or the discriminator. To improve the generator, MaliGAN BIBREF8 developed a normalized maximum likelihood optimization target for the generator to stably model the discrete sequences. guided the generator with reward signals leaked from the discriminator at all generation steps to deal with long text generation task",
      "The training objective of our generator INLINEFORM0 is derived from the objective of other discrete GANs with RL training method: DISPLAYFORM0   where INLINEFORM0 denotes the rewards from the discriminator INLINEFORM1 and the entropy regularized term INLINEFORM2 encourages INLINEFORM3 to generate diverse text samples. INLINEFORM4 is a temperature hyper-parameter to balance these two terms"
    ]
  },
  {
    "title": "Predicting the Industry of Users on Social Media",
    "answer": "They looked at Twitter.",
    "evidence": [
      "Introduction Over the past two decades, the emergence of social media has enabled the proliferation of traceable human behavior. The content posted by users can reflect who their friends are, what topics they are interested in, or which company they are working for. At the same time, users are listing a number of profile fields to define themselves to others",
      "Related Work Alongside the wide adoption of social media by the public, researchers have been leveraging the newly available data to create and refine models of users' behavior and profiling. There exists a myriad research that analyzes language in order to profile social media users. Some studies sought to characterize users' personality BIBREF9 , BIBREF10 , while others sequenced the expressed emotions BIBREF11 , studied mental disorders BIBREF12 , and the progression of health conditions BIBREF13",
      ". However, profile information can be limited, depending on the platform, or it is often deliberately omitted BIBREF3 . To uncloak this information, a number of studies have utilized social media users' footprints to approximate their profiles. This paper explores the potential of predicting a user's industry –the aggregate of enterprises in a particular field– by identifying industry indicative text in social media",
      ". Because of the manual annotation step, their method was not scalable; moreover, because they identified the occupation class inside a user self-description, only a very small fraction of the Twitter users could be included (in their case, 5,191 users). Both of these recent studies are based on micro-blogging platforms, which inherently restrict the number of characters that a post can have, and consequently the way that users can express themselves",
      ". The utilization of such metadata has proven important in facilitating further developments of applications in advertising BIBREF0 , personalization BIBREF1 , and recommender systems BIBREF2 . However, profile information can be limited, depending on the platform, or it is often deliberately omitted BIBREF3 . To uncloak this information, a number of studies have utilized social media users' footprints to approximate their profiles"
    ]
  },
  {
    "title": "Open Event Extraction from Online Text using a Generative Adversarial Network",
    "answer": "This model overcomes the assumption that all words in a document are generated from a single event by allowing the number of events to be learned automatically from data, as proposed in the Dirichlet Process Event Mixture Model (DPEMM).",
    "evidence": [
      "After the model training, the generator INLINEFORM0 learns the mapping function between the document-event distribution and the document-level event-related word distributions (entity, location, keyword and date). In other words, with an event distribution INLINEFORM1 as input, INLINEFORM2 could generate the corresponding entity distribution, location distribution, keyword distribution and date distribution",
      ". However, both LEM and DPEMM have two limitations: (1) they assume that all words in a document are generated from a single event which can be represented by a quadruple INLINEFORM0 entity, location, keyword, date INLINEFORM1 . However, long texts such news articles often describe multiple events which clearly violates this assumption; (2) During the inference process of both approaches, the Gibbs sampler needs to compute the conditional posterior distribution and assigns an event for each document",
      ". To address the limitation that LEM requires the number of events to be pre-set, Zhou et al. zhou2017event further proposed the Dirichlet Process Event Mixture Model (DPEMM) in which the number of events can be learned automatically from data. However, both LEM and DPEMM have two limitations: (1) they assume that all words in a document are generated from a single event which can be represented by a quadruple INLINEFORM0 entity, location, keyword, date INLINEFORM1",
      "Abstract To extract the structured representations of open-domain events, Bayesian graphical models have made some progress. However, these approaches typically assume that all words in a document are generated from a single event. While this may be true for short text such as tweets, such an assumption does not generally hold for long text such as news articles. Moreover, Bayesian graphical models often rely on Gibbs sampling for parameter inference which may take long time to converge",
      "the key event elements; (2) The generator INLINEFORM1 , as shown in the lower-left part of Figure FIGREF4 , generates a fake document INLINEFORM2 which is constituted by four multinomial distributions using an event distribution INLINEFORM3 drawn from a Dirichlet distribution as input; (3) The discriminator INLINEFORM4 , as shown in the lower-right part of Figure FIGREF4 , distinguishes the real documents from the fake ones and its output is subsequently employed as a learning signal to update the"
    ]
  },
  {
    "title": "Copenhagen at CoNLL--SIGMORPHON 2018: Multilingual Inflection in Context with Explicit Morphosyntactic Decoding",
    "answer": "MSD prediction is the task of predicting the morphosyntactic description (MSD) tag of the target form.",
    "evidence": [
      ". This observation explains the high results we obtain also for Track 2. Adding the auxiliary objective of MSD prediction has a variable effect: for four languages (de, en, es, and sv) the effect is positive, while for the rest it is negative. We consider this to be an issue of insufficient data for the training of the auxiliary component in the low resource setting we are working with",
      ". MSD tag predictions are conditioned on the context encoding, as described in UID15 . Tags are generated with an LSTM one component at a time, e.g. the tag PRO;NOM;SG;1 is predicted as a sequence of four components, INLINEFORM0 PRO, NOM, SG, 1 INLINEFORM1 . For every training instance, we backpropagate the sum of the main loss and the auxiliary loss without any weighting. As MSD tags are only available in Track 1, this augmentation only applies to this track",
      "MSD prediction Figure FIGREF32 summarises the average MSD-prediction accuracy for the multi-tasking experiments discussed above. Accuracy here is generally higher than on the main task, with the multilingual finetuned setup for Spanish and the monolingual setup for French scoring best: 66.59% and 65.35%, respectively. This observation illustrates the added difficulty of generating the correct surface form even when the morphosyntactic description has been identified correctly",
      ". The results indicate that encoding the full context with an LSTM highly enhances the performance of the model, by 11.15% on average. This observation explains the high results we obtain also for Track 2. Adding the auxiliary objective of MSD prediction has a variable effect: for four languages (de, en, es, and sv) the effect is positive, while for the rest it is negative",
      ". However, we now reduce the entire past context to a fixed-size vector by encoding it with a forward LSTM, and we similarly represent the future context by encoding it with a backwards LSTM. We introduce an auxiliary objective that is meant to increase the morpho-syntactic awareness of the encoder and to regularise the learning process—the task is to predict the MSD tag of the target form. MSD tag predictions are conditioned on the context encoding, as described in UID15"
    ]
  },
  {
    "title": "Exploiting Task-Oriented Resources to Learn Word Embeddings for Clinical Abbreviation Expansion",
    "answer": "They use 1,160 raw ICU texts to train the word embeddings.",
    "evidence": [
      "Baseline Models For our task, it's difficult to re-implement the supervised methods as in previous works mentioned since we do not have sufficient training data. And a direct comparison is also impossible because all previous work used different data sets which are not publicly available. Alternatively, we use the following baselines to compare with our approach. This baseline model chooses the highest rating candidate expansion in the domain specific knowledge base",
      ". Secondly, in many cases although we have the correct expansion in the candidate list, it's not ranked as the top one due to the lower semantic similarity because there are not enough samples in the training data. Among all the incorrect expansions in our test set, such kind of errors accounted for about 54%. One possible solution may be adding more effective data to the embedding training, which means discovering more task-oriented resources",
      ". Alternatively, we use the following baselines to compare with our approach. This baseline model chooses the highest rating candidate expansion in the domain specific knowledge base. Raw Input embeddings: We trained word embeddings only from the 1,160 raw ICU texts and we choose the most semantically related candidate as the answer",
      ". Experimental results show that the embeddings trained on the task-oriented corpus are much more useful than those trained on other corpora. By combining the embeddings with domain-specific knowledge, we achieve 82.27% accuracy, which outperforms baselines and is close to human's performance.",
      ". We use word2vec BIBREF0 to train the word embeddings. The dimension of embeddings is empirically set to 100. Since the goal of our task is to find the correct expansion for an abbreviation, we use accuracy as a metric to evaluate the performance of our approach. For ground-truth, we have 100 physician logs which are manually expanded and normalized by one of the authors Dr"
    ]
  },
  {
    "title": "Identifying and Understanding User Reactions to Deceptive and Trusted Social News Sources",
    "answer": "The nine types of user reactions are agreement, answer, appreciation, disagreement, elaboration, humor, negative reaction, question, and \"other\".",
    "evidence": [
      "Reaction Type Classification In this section, we describe our approach to classify user reactions into one of eight types of discourse: agreement, answer, appreciation, disagreement, elaboration, humor, negative reaction, or question, or as none of the given labels, which we call “other”, using linguistically-infused neural network models.",
      ". Deceptive sources are ranked by their intent to deceive as follows: clickbait (attention-grabbing, misleading, or vague headlines to attract an audience), conspiracy theory (uncorroborated or unreliable information to explain events or circumstances), propaganda (intentionally misleading information to advance a social or political agenda), and disinformation (fabricated or factually incorrect information meant to intentionally deceive readers)",
      "Reaction Type Classification Results As shown in Figure FIGREF7 , our linguistically-infused neural network model that relies solely on the content of the reaction and its parent has comparable performance to the more-complex CRF model by Zhang et al. zhang2017characterizing, which relies on content as well as additional metadata like the author, thread (e.g., the size of the the thread, the number of branches), structure (e.g., the position within the thread), and community (i.e",
      ". A breakdown of each dataset by source type is shown in Table TABREF10 . Figure FIGREF11 illustrates the distribution of deceptive news sources and reactions across the four sub-categories of deceptive news sources. In our analysis, we consider the set of all deceptive sources and the set excluding the most extreme (disinformation).",
      ". The vector representation sub-network consists of two dense layers. We incorporate information from both sub-networks through concatenated padded text sequences and vector representations of normalized Linguistic Inquiry and Word Count (LIWC) features BIBREF16 for the text of each post and its parent."
    ]
  },
  {
    "title": "Deep Semi-Supervised Learning with Linguistically Motivated Sequence Labeling Task Hierarchies",
    "answer": "Our model has 3 sources of error signals, indicating that it is supervised on 3 tasks.",
    "evidence": [
      "Supervision of Multiple Tasks Our model has 3 sources of error signals - one for each task. Since each task is categorical we use the discrete cross entropy to calculate the loss for each task: $ H(p, q) = - \\sum _{i}^{n_{labels}} p(label_i) \\ log \\ q(label_i) $  Where $n_{labels}$ is the number of labels in the task, $q(label_i)$ is the probability of label $i$ under the predicted distribution, and $p(label_i)$ is the probability of label $i$ in the true distribution (in this case, a one-hot vector)",
      ". Conversely, information about downstream tasks should also provide information that aids generalisation for junior downstream tasks, a form of semi-supervised learning. Arguably, there is a two-way relationship between each pair of tasks",
      ". Arguably, there is a two-way relationship between each pair of tasks. Following work such as sogaard2016deep, that exploits such hierarchies in a fully supervised setting, we represent this hierarchical relationship within the structure of a multi-task Recurrent Neural Network (RNN), where junior tasks in the hierarchy are supervised on inner layers and the parameters are jointly optimised during training",
      ". We also present results for our hierarchical model where there is no training on unlabelled data (but there is the LM) and confirm previous results that arranging tasks in a hierarchy improves performance. Results for both models can be seen for POS in Table 2 and for Chunk in Table 1 .",
      "Baseline Results We compare the results of our model to a baseline multi-task architecture inspired by yang2016multi. In our baseline model there are no explicit connections between tasks - the only shared parameters are in the hidden layer. We also present results for our hierarchical model where there is no training on unlabelled data (but there is the LM) and confirm previous results that arranging tasks in a hierarchy improves performance"
    ]
  },
  {
    "title": "Yoga-Veganism: Correlation Mining of Twitter Health Data",
    "answer": "We observe that keywords \"go\", \"day\" are repeated in Topic 2, Topic 3, and Topic 4, and keyword \"yoga\" is found in Topic 1 and Topic 4, indicating possible correlations between these topics.",
    "evidence": [
      ". A model with too many topics, is typically have many overlaps, small sized bubbles clustered in one region of the chart. In right hand side, the words represent the salient keywords. If we move the cursor over one of the bubbles (Fig. FIGREF21 ), the words and bars on the right-hand side have been updated and top-30 salient keywords that form the selected topic and their estimated term frequencies are shown. We observe interesting hidden correlation in data",
      "Methodology We use Twitter health-related data for this analysis. In subsections [subsec:3.1]3.1, [subsec:3.2]3.2, [subsec:3.3]3.3, and [subsec:3.4]3.4 elaborately present how we can infer the meaning of unstructured data. [subsec:3.5]3.5 shows how we do manual annotation for ground truth comparison. FIGREF6 shows the overall pipeline of correlation mining.",
      ". FIGREF21 ), the words and bars on the right-hand side have been updated and top-30 salient keywords that form the selected topic and their estimated term frequencies are shown. We observe interesting hidden correlation in data. Topic 2 contains top-4 co-occurring keywords \"vegan\", \"yoga\", \"job\", \"every_woman\" having the highest term frequency. We can infer different things from the topic that \"women usually practice yoga more than men\", \"women teach yoga and take it as a job\", \"Yogi follow vegan diet\"",
      ". Keywords \"go\", \"day\" both are repeated in Topic 2, Topic 3, and Topic 4 (Table TABREF13 ). In Table TABREF13 keyword \"yoga\" has been found both in Topic 1 and Topic 4. We also notice that keyword \"eat\" is in Topic 2 and Topic 3 (Table TABREF13 ). If the same keywords being repeated in multiple topics, it is probably a sign that the `k' is large though we achieve the highest coherence score in NMF for k=4. We use LDA model for our further analysis",
      ". Nowadays people usually share their interests, thoughts via discussions, tweets, status in social media (i.e. Facebook, Twitter, Instagram etc.). It's huge amount of data and it's not possible to go through all the data manually. We need to mine the data to get overall statistics and then we will also be able to find some interesting correlation of data. Several works have been done on prediction of social media content BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3 , BIBREF4 . Prieto et al"
    ]
  },
  {
    "title": "Learning Relational Dependency Networks for Relation Extraction",
    "answer": "They learn relations jointly within the RDN.",
    "evidence": [
      ". Joint learning appears to help in about half of the relations (8/14). Particularly, in person category, joint learning with gold standard outperforms their individual learning counterparts. This is due to the fact that some relations such as parents, spouse, siblings etc. are inter-related and learning them jointly indeed improves performance. Hence Q2 can be answered affirmatively for half the relations.",
      "Joint learning To address our next question, we assessed our pipeline when learning relations independently (i.e., individually) versus learning relations jointly within the RDN, displayed in Table TABREF22 . Recall and F1 are omitted for conciseness – the conclusions are the same across all metrics. Joint learning appears to help in about half of the relations (8/14). Particularly, in person category, joint learning with gold standard outperforms their individual learning counterparts",
      ". These questions are extremely interesting from a general machine learning perspective, but also critical to the NLP community. As we show empirically, some of the results such as human advice being useful in many relations and joint learning being beneficial in the cases where the relations are correlated among themselves are on the expected lines",
      "Advice Table TABREF26 shows the results of experiments that test the use of advice within the joint learning setting. The use of advice improves or matches the performance of using only joint learning. The key impact of advice can be mostly seen in the improvement of recall in several relations. This clearly shows that using human advice patterns allows us to extract more relations effectively making up for noisy or less number of training examples",
      ". As we show empirically, some of the results such as human advice being useful in many relations and joint learning being beneficial in the cases where the relations are correlated among themselves are on the expected lines. However, some surprising observations include the fact that weak supervision is not as useful as expected and word2vec features are not as predictive as the other domain-specific features. We first present the proposed pipeline with all the different components of the learning system"
    ]
  },
  {
    "title": "Simultaneous Neural Machine Translation using Connectionist Temporal Classification",
    "answer": "They use BLEU, RIBES, and token-level delay as evaluation metrics.",
    "evidence": [
      ". We conduct experiments in English-to-Japanese simultaneous translation with the proposed and baseline methods and show the proposed method achieves a good translation performance with relatively small latency. The proposed method can determine when to wait or translate in an adaptive manner and is useful in simultaneous translation tasks.",
      ". For evaluation metrics, we used BLEU BIBREF8 and RIBES BIBREF9 to measure translation accuracy, and token-level delay to measure latency. We used Kytea BIBREF10 as a tokenize method for evaluations of Japanese translation accuracy.",
      "Introduction Simultaneous translation is a translation task where the translation process starts before the end of an input. It helps real-time spoken language communications such as human conversations and public talks. A usual machine translation system works in the sentence level and starts its translation process after it reads the end of a sentence",
      "Experiments We conducted simultaneous translation experiments from English to Japanese and discussed accuracy, latency, and issues for translation results.",
      ". fujita13interspeech proposed a phrase-based approach to the simultaneous translation based on phrasal reordering probabilities. oda-etal-2015-syntax proposed a syntax-based method to determine when to start translation of observed inputs. Such an approach faces a trade-off between speed and accuracy; reducing the translation latency using very limited context information also causes the loss in the translation accuracy"
    ]
  },
  {
    "title": "Civique: Using Social Media to Detect Urban Emergencies",
    "answer": "The classifier model used for emergency detection is a combination of Support Vector Machines (SVM) and Naive Bayes (NB), with SVM chosen for stage one classification due to a better F-score.",
    "evidence": [
      "Emergency Classification The first classifier model acts as a filter for the second stage of classification. We use both SVM and NB to compare the results and choose SVM later for stage one classification model, owing to a better F-score. The training is performed on tweets labeled with classes <emergency>, and <non-emergency> based on unigrams as features",
      ". It shows that our classifier model is trained on appropriate words, which are very close to the emergency situations viz. “fire”, “earthquake”, “accident”, “break” (Unigram representation here, but possibly occurs in a bigram phrase with “fire”) etc. In figure FIGREF24 , the word cloud represents the word “respond” as the most frequently occurring word as people need urgent help, and quick response from the assistance teams.",
      ". We use the manually labeled data for training our classifiers. We use traditional classification techniques such as Support Vector Machines(SVM), and Naive Bayes(NB) for training, and perform 10-fold cross validation to obtain f-scores. Later, in real time, our system uses the Twitter streaming APIs to get data, pre-processes it using the same modules, and detects emergencies using the classifiers built above",
      ". Our focus is on detecting urban emergencies as events from Twitter messages. We classify events ranging from natural disasters to fire break outs, and accidents. Our system detects whether a tweet, which contains a keyword from a pre-decided list, is related to an actual emergency or not. It also classifies the event into its appropriate category, and visualizes the possible location of the emergency event on the map",
      ". We also perform attribute evaluation for the classification model, and create a word cloud based on the output values, shown in figure FIGREF24 . It shows that our classifier model is trained on appropriate words, which are very close to the emergency situations viz. “fire”, “earthquake”, “accident”, “break” (Unigram representation here, but possibly occurs in a bigram phrase with “fire”) etc"
    ]
  },
  {
    "title": "Identifying Products in Online Cybercrime Marketplaces: A Dataset for Fine-grained Domain Adaptation",
    "answer": "Three people annotated every post in the Darkode training, Hack Forums training, Blackhat test, and Nulled test sets.",
    "evidence": [
      "Annotation Process We developed our annotation guidelines through six preliminary rounds of annotation, covering 560 posts. Each round was followed by discussion and resolution of every post with disagreements. We benefited from members of our team who brought extensive domain expertise to the task. As well as refining the annotation guidelines, the development process trained annotators who were not security experts. The data annotated during this process is not included in Table TABREF3",
      ". In total, we annotated 130,336 tokens; accounting for multiple annotators, our annotators considered 478,176 tokens in the process of labeling the data. Figure FIGREF2 shows two examples of posts from Darkode. In addition to aspects of the annotation, which we describe below, we see that the text exhibits common features of web text: abbreviations, ungrammaticality, spelling errors, and visual formatting, particularly in thread titles",
      ". The data annotated during this process is not included in Table TABREF3 . Once we had defined the annotation standard, we annotated datasets from Darkode, Hack Forums, Blackhat, and Nulled as described in Table TABREF3 . Three people annotated every post in the Darkode training, Hack Forums training, Blackhat test, and Nulled test sets; these annotations were then merged into a final annotation by majority vote",
      ". We performed annotation on the tokenized data so that annotations would be consistent with surrounding punctuation and hyphenated words. Our full annotation guide is available with our data release. Our basic annotation principle is to annotate tokens when they are either the product that will be delivered or are an integral part of the method leading to the delivery of that product. Figure FIGREF2 shows examples of this for a deliverable product (bot) as well as a service (cleaning)",
      "Type-level Annotation Another approach following Hypothesis 1 is to use small amounts of supervised data, One cheap approach for annotating data in a new domain is to exploit type-level annotation BIBREF12 , BIBREF13 . Our token-level annotation standard is relatively complex to learn, but a researcher could quite easily provide a few exemplar products for a new forum based on just a few minutes of reading posts and analyzing the forum"
    ]
  },
  {
    "title": "Deep Health Care Text Classification",
    "answer": "The variant of Recurrent Neural Network (RNN) used is LSTM (Long Short-Term Memory).",
    "evidence": [
      "Recurrent neural network (RNN) and it’s variant Recurrent neural network (RNN) was an enhanced model of feed forward network (FFN) introduced in 1990 BIBREF10 . The input sequences ${x_T}$ of arbitrary length are passed to RNN and a transition function $tf$ maps them into hidden state vector $h{i_{t - 1}}$ recursively. The hidden state vector $h{i_{t - 1}}$ are calculated based on the transition function $tf$ of present input sequence ${x_T}$ and previous hidden state vector $h{i_{t - 1}}$",
      ". For each experiment, learning rate is set to 0.01. An experiment with embedding size 512 performed well in both the RNN and LSTM networks. Thus for the rest of the experiments embedding size is set to 512. The embedding layer output vector is further passed to RNN and its variant LSTM layer. RNN and LSTM obtain the optimal feature representation and those feature representation are passed to the dropout layer. Dropout layer contains 0.1 which removes the neurons and its connections randomly",
      "Results All experiments are trained using backpropogation through time (BPTT) BIBREF14 on Graphics processing unit (GPU) enabled TensorFlow BIBREF6 computational framework in conjunction with Keras framework in Ubuntu 14.04. We have submitted one run based on LSTM for task 1 and two runs composed of one run based on RNN and other one based on LSTM for task 2. The evaluation results is given by shared task committee are reported in Table 3 and 4.",
      ". For each task, two systems are built and that classify the tweet at the tweet level. RNN and LSTM are used for extracting features and non-linear activation function at the last layer facilitates to distinguish the tweets of different categories. The experiments are conducted on 2nd Social Media Mining for Health Applications Shared Task at AMIA 2017. The experiment results are considerable; however the proposed method is appropriate for the health text classification",
      ". RNN and LSTM obtain the optimal feature representation and those feature representation are passed to the dropout layer. Dropout layer contains 0.1 which removes the neurons and its connections randomly. This acts as a regularization parameter. In task 1 the output layer contains $sigmoid$ activation function and $softmax$ activation function for task 2."
    ]
  },
  {
    "title": "MultiFC: A Real-World Multi-Domain Dataset for Evidence-Based Fact Checking of Claims",
    "answer": "The base model used in the experiment is described in Section SECREF16.",
    "evidence": [
      ". Here, we have different domains, as opposed to conceptually different tasks, but use their framework, as we have the same underlying problem of disparate label spaces. A more formal problem definition follows next, as our evidence ranking and veracity prediction model in Section SECREF22 We frame our problem as a multi-task learning one, where access to labelled datasets for INLINEFORM0 tasks INLINEFORM1 is given at training time with a target task INLINEFORM2 that is of particular interest",
      ". In the later experimental descriptions, we refer to the part of the dataset crawled from a specific fact checking website as a domain, and we refer to each website as source. From each source, we crawled the ID, claim, label, URL, reason for label, categories, person making the claim (speaker), person fact checking the claim (checker), tags, article title, publication date, claim date, as well as the full text that appears when the claim is clicked",
      ". In addition, claim metadata (speaker, checker, linked entities) is optionally encoded for both categories of models, and ablation studies with and without that metadata are shown. We first describe the base model used in Section SECREF16 , followed by introducing our novel evidence ranking and veracity prediction model in Section SECREF22 , and lastly the metadata encoding model in Section SECREF31 .",
      "Dataset Construction We crawled a total of 43,837 claims with their metadata (see details in Table TABREF1 ). We present the data collection in terms of selecting sources, crawling claims and associated metadata (Section SECREF9 ); retrieving evidence pages; and linking entities in the crawled claims (Section SECREF13 ).",
      "Results For each domain, we compute the Micro as well as Macro F1, then mean average results over all domains. Core results with all vs. no metadata are shown in Table TABREF30 . We first experiment with different base model variants and find that label embeddings improve results, and that the best proposed models utilising multiple domains outperform single-task models (see Table TABREF36 ). This corroborates the findings of BIBREF30 . Per-domain results with the best model are shown in Table TABREF34"
    ]
  },
  {
    "title": "Retrieval-based Goal-Oriented Dialogue Generation",
    "answer": "They compared their proposed model with the exemplar model and BIBREF0, which uses information of the belief state of the conversation as extra features.",
    "evidence": [
      ". Finally, we compare our retrieval-based model to various semantically conditioned models explicitly using past dialog act information, and find that our proposed model is competitive with the current state of the art (Chen et al., 2019), while not requiring explicit labels about past machine acts.",
      ". We looked at the responses generated by our proposed models that had the highest score of these metrics and compared to the response generated by the baseline for that same dialogue. Overall we found that the baseline models tend to generate responses containing slots and values for the wrong domain. In addition, by examining the outputs we could see that the vector extrema metric is very sensitive when it comes to slight differences in the references and prediction",
      ". Finally, based on our human evaluations, we show that a simple retrieval step leads to system responses that are more fluent and appropriate than the responses generated by the hierarchical encoder-decoder model.",
      "Discussion As shown in table TABREF7, our simplest proposed model achieved the largest improvements over the baseline when it came to the average embedding similarity and vector extrema similarity. As it is hard to interpret what the difference in performance of each model is based on standard dialogue metrics we examined the output to spot the major differences in response generation of our proposed models versus the baseline",
      ". This was confirmed by the human evaluations and also the Inform/Request metrics. When comparing the performance of the exemplar-based model to models that do not use information about past acts to condition the decoder, we observe that including a simple retrieval step leads to very large gains in the success of providing the inform/request slots.. In addition, the exemplar model performs better than BIBREF0, which uses information of the belief state of the conversation as extra features"
    ]
  },
  {
    "title": "User Generated Data: Achilles' heel of BERT",
    "answer": "The reason behind the drop in performance using BERT for some popular tasks, such as sentiment analysis and textual similarity, is the introduction of noise, specifically spelling mistakes, in the input text data.",
    "evidence": [
      ". Also, from the plots it is evident that the reason for this drop in performance is introduction of noise (spelling mistakes). After all we get very good numbers, for each of the three tasks, when there is no error (0.0 % error). To understand the reason behind the drop in performance, first we need to understand how BERT processes input text data. BERT uses WordPiece tokenizer to tokenize the text. WordPiece tokenizer utterances based on the longest prefix matching algorithm to generate tokens",
      ". These use cases are known to have much more noise in the data as compared to benchmark datasets. In this work we systematically show that when the data is noisy, there is a significant degradation in the performance of BERT. Specifically, we performed experiments using BERT on popular tasks such sentiment analysis and textual similarity. For this we work with three well known datasets - IMDB movie reviews, SST-2 and STS-B to measure the performance",
      "Results ::: Key Findings It is clear from the above plots that as we increase the percentage of error, for each of the three tasks, we see a significant drop in BERT’s performance. Also, from the plots it is evident that the reason for this drop in performance is introduction of noise (spelling mistakes). After all we get very good numbers, for each of the three tasks, when there is no error (0.0 % error)",
      ". This misrepresentation in turn impacts the performance of downstream subcomponents of BERT bringing down the overall performance of BERT model. Hence, as we systematically introduce more errors, the quality of output of the tokenizer degrades further, resulting in the overall performance drop. Our results and analysis shows that one cannot apply BERT blindly to solve NLP problems especially in industrial settings",
      ". Specifically, we performed experiments using BERT on popular tasks such sentiment analysis and textual similarity. For this we work with three well known datasets - IMDB movie reviews, SST-2 and STS-B to measure the performance. Further, we examine the reason behind this performance drop and identify the shortcomings in the BERT pipeline."
    ]
  },
  {
    "title": "Cohesion and Coalition Formation in the European Parliament: Roll-Call Votes and Twitter Activities",
    "answer": "The analysis provides general insights into the cohesion of political groups in the Eighth European Parliament, both overall and across different policy areas.",
    "evidence": [
      "Cohesion of political groups In this section we first report on the level of cohesion of the European Parliament's groups by analyzing the co-voting through the agreement and ERGM measures. Next, we explore two important policy areas, namely Economic and monetary system and State and evolution of the Union. Finally, we analyze the cohesion of the European Parliament's groups on Twitter. Existing research by Hix et al",
      ". We measure the co-voting cohesion of the political groups in the Eighth European Parliament using Krippendorff's Alpha—the results are shown in Fig FIGREF30 (panel Overall). The Greens-EFA have the highest cohesion of all the groups. This finding is in line with an analysis of previous compositions of the Fifth and Sixth European Parliaments by Hix and Noury BIBREF11 , and the Seventh by VoteWatch BIBREF37 . They are closely followed by the S&D and EPP",
      ". Even though these two methodologies come with two different sets of techniques and are based on different assumptions, they provide consistent results. The main contributions of this paper are as follows: (i) We give general insights into the cohesion of political groups in the Eighth European Parliament, both overall and across different policy areas. (ii) We explore whether coalitions are formed in the same way for different policy areas",
      ". There are numerous studies reported in the literature that successfully correlate social-media activities to phenomena like election outcomes BIBREF0 , BIBREF1 or stock-price movements BIBREF2 , BIBREF3 . In this paper we study the cohesion and coalitions exhibited by political groups in the Eighth European Parliament (2014–2019). We analyze two entirely different aspects of how the Members of the European Parliament (MEPs) behave in policy-making processes",
      ". BIBREF10 , BIBREF13 , BIBREF11 shows that the cohesion of the European political groups has been rising since the 1990s, and the level of cohesion remained high even after the EU's enlargement in 2004, when the number of MEPs increased from 626 to 732. We measure the co-voting cohesion of the political groups in the Eighth European Parliament using Krippendorff's Alpha—the results are shown in Fig FIGREF30 (panel Overall). The Greens-EFA have the highest cohesion of all the groups"
    ]
  },
  {
    "title": "S-Net: From Answer Extraction to Answer Generation for Machine Reading Comprehension",
    "answer": "The MS-MARCO dataset differs from SQuAD in that it contains multiple passages for a question, whereas SQuAD has only one passage, and the answer in MS-MARCO is not necessarily a sub-span of the passages.",
    "evidence": [
      ". Second, the SQuAD dataset only has one passage for a question, while the MS-MARCO dataset contains multiple passages. Existing methods for the MS-MARCO dataset usually follow the extraction based approach for single passage in the SQuAD dataset. It formulates the task as predicting the start and end positions of the answer in the passage",
      ". There are two main differences in existing machine reading comprehension datasets. First, the SQuAD dataset constrains the answer to be an exact sub-span in the passage, while words in the answer are not necessary in the passages in the MS-MARCO dataset. Second, the SQuAD dataset only has one passage for a question, while the MS-MARCO dataset contains multiple passages. Existing methods for the MS-MARCO dataset usually follow the extraction based approach for single passage in the SQuAD dataset",
      ". The answer in SQuAD is a segment of text, or span, from the corresponding reading passage. Similar to the SQuAD, MS-MARCO BIBREF1 is the reading comprehension dataset which aims to answer the question given a set of passages. The answer in MS-MARCO is generated by human after reading all related passages and not necessarily sub-spans of the passages. To the best of our knowledge, the existing works on the MS-MARCO dataset follow their methods on the SQuAD",
      ". Existing methods for the MS-MARCO dataset usually follow the extraction based approach for single passage in the SQuAD dataset. It formulates the task as predicting the start and end positions of the answer in the passage. However, as defined in the MS-MARCO dataset, the answer may come from multiple spans, and the system needs to elaborate the answer using words in the passages and words from the questions as well as words that cannot be found in the passages or questions",
      "Result Table 2 shows the results on the MS-MARCO test data. Our extraction model achieves 41.45 and 44.08 in terms of ROUGE-L and BLEU-1, respectively. Next we train the model 30 times with the same setting, and select models using a greedy search. We sum the probability at each position of each single model to decide the ensemble result. Finally we select 13 models for ensemble, which achieves 42.92 and 44"
    ]
  },
  {
    "title": "Neural Machine Translation with Supervised Attention",
    "answer": "The dataset used is from the NIST2008 Open Machine Translation Campaign.",
    "evidence": [
      ".2 BLEU points on test sets over its direct baseline NMT2. It is clear from these results that our supervised attention mechanism is highly effective in practice. As explained in §2, standard NMT can not use the target word information to predict its aligned source words, and thus might fail to predict the correct source words for some target words",
      ".2. To explore their behaviors on the development set, we employed the GIZA++ to generate the alignment on the training set prior to the training SA-NMT. In Table TABREF21 , we can see that MUL is better than MSE. Furthermore, CE performs best among all losses, and thus we adopt it for the following experiments. In addition, we also run fast_align to generate alignments as the supervision for SA-NMT and the results were reported in Table TABREF22",
      ". Specifically, except the stopping iteration which was selected using development data, we used the default settings set out in BIBREF0 for all NMT-based systems: the dimension of word embedding was 620, the dimension of hidden units was 1000, the batch size was 80, the source and target side vocabulary sizes were 30000, the maximum sequence length was 50, the beam size for decoding was 12, and the optimization was done by Adadelta with all hyper-parameters suggested by BIBREF16",
      "The Large Scale Translation Task We used the data from the NIST2008 Open Machine Translation Campaign. The training data consisted of 1.8M sentence pairs, the development set was nist02 (878 sentences), and the test sets are were nist05 (1082 sentences), nist06 (1664 sentences) and nist08 (1357 sentences). We compared the proposed approach with three strong baselines:  Moses: a phrase-based machine translation system BIBREF15 ;  NMT1: an attention based NMT BIBREF0 system at https://github",
      ". We find that both standard NMT generally outperforms Moses except NMT1 on nist05. The proposed SA-NMT achieves significant and consistent improvements over all three baseline systems, and it obtains the averaged gains of 2.2 BLEU points on test sets over its direct baseline NMT2. It is clear from these results that our supervised attention mechanism is highly effective in practice"
    ]
  },
  {
    "title": "An Unsupervised Word Sense Disambiguation System for Under-Resourced Languages",
    "answer": "The adjusted Rand index (ARI) proposed by Hubert is used as an evaluation measure.",
    "evidence": [
      "The original SemEval 2010 Task 14 used the V-Measure external clustering measure BIBREF5 . However, this measure is maximized by clustering each sentence into his own distinct cluster, i.e., a `dummy' singleton baseline. This is achieved by the system deciding that every ambiguous word in every sentence corresponds to a different word sense. To cope with this issue, we follow a similar study BIBREF1 and use instead of the adjusted Rand index (ARI) proposed by Hubert:85 as an evaluation measure",
      ". The sparse mode uses the traditional vector space model to estimate the most similar word sense corresponding to its context. The dense mode, instead, uses synset embeddings to cope with the sparsity problem. We describe the architecture of the present system and also conduct its evaluation on three different lexical semantic resources for Russian. We found that the dense mode substantially outperforms the sparse one on all datasets according to the adjusted Rand index.",
      ". The column Avg. consists of the weighted average of the datasets w.r.t. We observe that the SenseGram-based approach for word sense disambiguation yields substantially better results in every case (Table TABREF25 ). The primary reason for that is the implicit handling of similar words due to the averaging of dense word vectors for semantically related words. Thus, we recommend using the dense approach in further studies",
      ". In the gold standard, each word is provided with a set of instances, i.e., the sentences containing the word. Each instance is manually annotated with the single sense identifier according to a pre-defined sense inventory. Each participating system estimates the sense labels for these ambiguous words, which can be viewed as a clustering of instances, according to sense labels. The system's clustering is compared to the gold-standard clustering for evaluation.",
      ". Pelevina:16 proposed a similar approach to WSD, but based on dense vector representations (word embeddings), called SenseGram. Similarly to SenseGram, our WSD system is based on averaging of word embeddings on the basis of an automatically induced sense inventory. A crucial difference, however, is that we induce our sense inventory from synonymy dictionaries and not distributional word vectors"
    ]
  },
  {
    "title": "Self-Attentional Models Application in Task-Oriented Dialogue Generation Systems",
    "answer": "We use the Dialogue State Tracking Competition 2 (DSTC2) dataset, M2M-sim-M (in the movie domain), and M2M-sim-R (in the restaurant domain) for training the models.",
    "evidence": [
      "Experiments ::: Datasets We use three different datasets for training the models. We use the Dialogue State Tracking Competition 2 (DSTC2) dataset BIBREF27 which is the most widely used dataset for research on task-oriented chatbots. We also used two other datasets recently open-sourced by Google Research BIBREF28 which are M2M-sim-M (dataset in movie domain) and M2M-sim-R (dataset in restaurant domain)",
      ". F1-Entity Score: Datasets used in task-oriented chores have a set of entities which represent user preferences. For example, in the restaurant domain chatbots common entities are meal, restaurant name, date, time and the number of people (these are usually the required entities which are crucial for making reservations, but there could be optional entities such as location or rating). Each target response has a set of entities which the system asks or informs the user about",
      ". The M2M dataset has more diversity in both language and dialogue flow compared to the the commonly used DSTC2 dataset which makes it appealing for the task of creating task-oriented chatbots. This is also the reason that we decided to use M2M dataset in our experiments to see how well models can handle a more diversed dataset.",
      "Experiments ::: Datasets ::: Dataset Preparation We followed the data preparation process used for feeding the conversation history into the encoder-decoder as in BIBREF5. Consider a sample dialogue $D$ in the corpus which consists of a number of turns exchanged between the user and the system. $D$ can be represented as ${(u_1, s_1),(u_2, s_2), ...,(u_k, s_k)}$ where $k$ is the number of turns in this dialogue",
      ". The F1 score is the most commonly used metric used for the evaluation of named-entity recognition models which is the harmonic average of precision and recall of the model. We calculate this metric by micro-averaging over all the system generated responses."
    ]
  },
  {
    "title": "\"How May I Help You?\": Modeling Twitter Customer Service Conversations Using Fine-Grained Dialogue Acts",
    "answer": "The dialogue acts that capture fine-grained information, such as those that can convey overlapping and nuanced conversational intentions, are more suited to the Twitter domain.",
    "evidence": [
      ". We focus on the customer service domain on Twitter, which has not previously been explored in the context of dialogue act classification. In this new domain, we can provide meaningful recommendations about good communicative practices, based on real data. Our methodology pipeline is shown in Figure FIGREF2 .",
      ". In this work, we are interested in the dialogic characteristics of Twitter conversations, rather than speech acts in stand-alone tweets. Different dialogue act taxonomies have been developed to characterize conversational acts. Core and Allen present the Dialogue Act Marking in Several Layers (DAMSL), a standard for discourse annotation that was developed in 1997 BIBREF0",
      ". They describe good features for speech act classification and the application of such a system to detect stories on social media BIBREF11 . In this work, we are interested in the dialogic characteristics of Twitter conversations, rather than speech acts in stand-alone tweets. Different dialogue act taxonomies have been developed to characterize conversational acts",
      ". We address various different challenges: dialogue act annotated data is not available for customer service on Twitter, the task of dialogue act annotation is subjective, existing taxonomies do not capture the fine-grained information we believe is valuable to our task, and tweets, although concise in nature, often consist of overlapping dialogue acts to characterize their full intent",
      ". It is important to note at this point that we make an important choice as to how we will handle dialogue act tagging for each turn. We note that each turn may contain more than one dialogue act vital to carry its full meaning. Thus, we choose not to carry out a specific segmentation task on our tweets, contrary to previous work BIBREF24 , BIBREF25 , opting to characterize each tweet as a single unit composed of different, often overlapping, dialogue acts"
    ]
  },
  {
    "title": "Sex Trafficking Detection with Ordinal Regression Neural Networks",
    "answer": "The lexicon of trafficking flags can be expanded through word embeddings and t-SNE, as well as by periodically re-training the skip-gram model and updating the emoji map on new escort ads.",
    "evidence": [
      ". Law enforcement maintains a lexicon of trafficking flags mapping certain emojis to their potential true meanings (e.g., the cherry emoji can indicate an underaged victim), but compiling such a lexicon manually is expensive, requires frequent updating, and relies on domain expertise that is hard to obtain (e.g., insider information from traffickers or their victims)",
      ". The proposed method significantly improves on the previous state-of-the-art on Trafficking-10K, an expert-annotated dataset of escort ads. Additionally, because traffickers use acronyms, deliberate typographical errors, and emojis to replace explicit keywords, we demonstrate how to expand the lexicon of trafficking flags through word embeddings and t-SNE.",
      ". The emoji map can assist anti-trafficking experts in expanding the existing lexicon of trafficking flags. For example, according to the lexicon we obtained from Global Emancipation Network, the cherry emoji and the lollipop emoji are both flags for underaged victims",
      ". If we re-train the skip-gram model and update the emoji map periodically on new escort ads, when traffickers switch to new emojis, the map can link the new emojis to the old ones, assisting anti-trafficking experts in expanding the lexicon of trafficking flags. This approach also works for acronyms and deliberate typos.",
      "Emoji Analysis The fight against human traffickers is adversarial and dynamic. Traffickers often avoid using explicit keywords when advertising victims, but instead use acronyms, intentional typos, and emojis BIBREF9 . Law enforcement maintains a lexicon of trafficking flags mapping certain emojis to their potential true meanings (e.g"
    ]
  },
  {
    "title": "MDE: Multi Distance Embeddings for Link Prediction in Knowledge Graphs",
    "answer": "The experiments were conducted on four standard datasets: WN18, FB15k, WN18RR, and FB15k-237.",
    "evidence": [
      ". This problem is crucial because the ranking results of the models are close and the reported works make an unfair comparison of the dataset construction rather than the relational learning models. This is considerable when comparing results to ConvE that generates all possible combinations of object entities to generate samples(e.g., we observed in each iteration, it generates $\\approx 26000$ negative samples per one positive sample when training on WN18RR)",
      ". Here, we use 1 negative per positive, for MDE. To show the effect of dataset construction we compare this models with an experiment of TransE and RotatE on FB15k-237 that applies 256 negative samples per one positive sample BIBREF11 . Although these models perform better than the TransE on FB15K(Table1), they produce lower rankings on FB15k-237(Table2) in the more fair comparison conditions.",
      "Experiments Datasets: We experimented on four standard datasets: WN18 and FB15k are extracted by BIBREF5 from Wordnet BIBREF32 We used the same train/valid/test sets as in BIBREF5 . WN18 contains 40,943 entities, 18 relations and 141,442 train triples. FB15k contains 14,951 entities, 1,345 relations and 483,142 train triples",
      ". The ranges of the hyperparameters are set as follows: embedding dimension 25, 50, 100, batch size 100, 150, iterations 1000, 1500, 2500, 3600. We set the initial learning rate on all datasets to 10. The best embedding size and $\\gamma _1$ and $\\gamma _2$ and $\\beta _1$ and $\\beta _2$ values on WN18 were 50 and 1.9, 1.9, 2 and 1 respectively and for FB15k were 100, 14, 14, 1, 1. The best found embedding size and $\\gamma _1$ and $\\gamma _2$ and $\\beta _1$ and $\\beta _2$ values on FB15k-237 were 100, 5.6, 5",
      ". Negative Sampling and Data Augmentation Models: Currently, the training datasets for link prediction evaluation miss negative samples. Therefore, models generate their own negative samples while training. In consequence, the method of negative sample generation influences the result of the models. This problem is crucial because the ranking results of the models are close and the reported works make an unfair comparison of the dataset construction rather than the relational learning models"
    ]
  },
  {
    "title": "Gunrock: A Social Bot for Complex and Engaging Long Conversations",
    "answer": "Users' backstory queries about Gunrock are positively correlated to user satisfaction.",
    "evidence": [
      ". In this paper, we introduce some innovative system designs and related validation analysis. Overall, we found that users produce longer sentences to Gunrock, which are directly related to users' engagement (e.g., ratings, number of turns). Additionally, users' backstory queries about Gunrock are positively correlated to user satisfaction. Finally, we found dialog flows that interleave facts and personal opinions and stories lead to better user satisfaction.",
      "Analysis ::: Gunrock's Backstory and Persona We assessed the user's interest in Gunrock by tagging instances where the user triggered Gunrock's backstory (e.g., “What's your favorite color?\"). For users with at least one backstory question, we modeled overall (log) Rating with a linear regression by the (log) `Number of Backstory Questions Asked' (log transformed due to the variables' nonlinear relationship)",
      ". Analysis shows that users' speech behavior reflects these capabilities. Longer sentences and more questions about Gunrocks's backstory positively correlate with user experience. Additionally, we find evidence for interleaved dialog flow, where combining factual information with personal opinions and stories improve user satisfaction",
      ". We hypothesized that users who show greater curiosity about Gunrock will display higher overall ratings for the conversation based on her responses. Overall, the number of times users queried Gunrock's backstory was strongly related to the rating they gave at the end of the interaction (log:$\\beta $=0.10, SE=0.002, t=58.4, p$<$0.001)(see Figure 3). This suggests that maintaining a consistent personality — and having enough responses to questions the users are interested in — may improve user satisfaction.",
      ".g., “What's your favorite color?\"). For users with at least one backstory question, we modeled overall (log) Rating with a linear regression by the (log) `Number of Backstory Questions Asked' (log transformed due to the variables' nonlinear relationship). We hypothesized that users who show greater curiosity about Gunrock will display higher overall ratings for the conversation based on her responses"
    ]
  },
  {
    "title": "Image Captioning: Transforming Objects into Words",
    "answer": "The common captioning metrics used in the image captioning literature are CIDEr-D, SPICE, BLEU, METEOR, and ROUGE-L.",
    "evidence": [
      ". The Karpathy test and validation sets contain 5K images each. We evaluate our models using the CIDEr-D BIBREF22 , SPICE BIBREF23 , BLEU BIBREF24 , METEOR BIBREF25 , and ROUGE-L BIBREF26 metrics. While it has been shown experimentally that BLEU and ROUGE have lower correlation with human judgments than the other metrics BIBREF23 , BIBREF22 , the common practice in the image captioning literature is to report all the mentioned metrics.",
      "Introduction Image captioning—the task of providing a natural language description of the content within an image—lies at the intersection of computer vision and natural language processing. As both of these research areas are highly active and have experienced many recent advances, the progress in image captioning has naturally followed suit. On the computer vision side, improved convolutional neural network and object detection architectures have contributed to improved image captioning systems",
      ". In addition, it is interesting to see a significant improvement in the Count subcategory of SPICE, from 11.30 to 17.51. Image captioning methods in general show a large deficit in Count scores when compared with humans BIBREF23 , while we're able to show a significant improvement by adding explicit positional information. Some example images and captions illustrating these improvements are presented in Section SECREF29 .",
      "Dataset and Metrics We trained and evaluated our algorithm on the Microsoft COCO (MS-COCO) 2014 We report results on the Karpathy validation and test splits BIBREF8 , which are commonly used in other image captioning publications. The dataset contains 113K training images with 5 human annotated captions for each image. The Karpathy test and validation sets contain 5K images each",
      ". On the natural language processing side, more sophisticated sequential models, such as attention-based recurrent neural networks, have similarly resulted in more accurate image caption generation. Inspired by neural machine translation, most conventional image captioning systems utilize an encoder-decoder framework, in which an input image is encoded into an intermediate representation of the information contained within the image, and subsequently decoded into a descriptive text sequence"
    ]
  },
  {
    "title": "RC-QED: Evaluating Natural Language Derivations in Multi-Hop Reading Comprehension",
    "answer": "The dataset was annotated using a crowdsourcing framework, where 3 crowdworkers were asked to evaluate whether a set of Natural Language Descriptions (NLDs) can lead to a given statement at four scale levels.",
    "evidence": [
      "Data collection for RC-QED@!START@$^{\\rm E}$@!END@ ::: Results ::: Quality To evaluate the quality of annotation results, we publish another CS task on AMT. We randomly sample 300 True and Likely responses in this evaluation. Given NLDs and a statement, 3 crowdworkers are asked if the NLDs can lead to the statement at four scale levels",
      "Related work ::: RC datasets with explanations There exists few RC datasets annotated with explanations (Table TABREF50). The most similar work to ours is Science QA dataset BIBREF21, BIBREF22, BIBREF23, which provides a small set of NLDs annotated for analysis purposes. By developing the scalable crowdsourcing framework, our work provides one order-of-magnitude larger NLDs which can be used as a benchmark more reliably",
      ". Several datasets are annotated with introspective explanations, ranging from textual entailments BIBREF8 to argumentative texts BIBREF26, BIBREF27, BIBREF33. All these datasets offer the classification task of single sentences or sentence pairs. The uniqueness of our dataset is that it measures a machine's ability to extract relevant information from a set of documents and to build coherent logical reasoning steps.",
      ". The developed crowdsourcing annotation framework can be used for annotating other QA datasets with derivations. Through an experiment using two baseline models, we highlight several challenges of RC-QED. We will make the corpus of reasoning annotations and the baseline system publicly available at https://naoya-i.github.io/rc-qed/.",
      ". We randomly sampled 10,000 instances from 43,738 training instances and 2,000 instances from 5,129 validation instances (i.e. 36,000 annotation tasks were published on AMT). We manually converted structured WikiHop question-answer pairs (e.g. locatedIn(Macchu Picchu, Peru)) into natural language statements (Macchu Picchu is located in Peru) using a simple conversion dictionary. We use supporting documents provided by WikiHop"
    ]
  },
  {
    "title": "Word Embeddings to Enhance Twitter Gang Member Profile Identification",
    "answer": "The ground truth of gang membership in this dataset is established through a data collection process involving location-neutral keywords used by gang members, with an expanded search of their retweet, friends, and follower networks, which led to identifying 400 authentic gang member profiles on Twitter.",
    "evidence": [
      ". A data collection process involving location neutral keywords used by gang members, with an expanded search of their retweet, friends and follower networks, led to identifying 400 authentic gang member profiles on Twitter. Our study discovered that the text in their tweets and profile descriptions, their emoji use, their profile images, and music interests embodied by links to YouTube music videos, can help a classifier distinguish between gang and non-gang member profiles",
      ". The diverse set of features, chosen to combat the fact that gang members often use local terms and hashtags in their posts, offered encouraging results. In this paper, we report our experience in integrating deep learning into our gang member profile classifier. Specifically, we investigate the effect of translating the features into a vector space using word embeddings BIBREF10",
      ".S. in their Twitter profiles. The dataset was further expanded by examining the friends, follower, and retweet networks of the gang member profiles found by searching for seed words. Specific details about our data curation procedure are discussed in BIBREF9 . Ultimately, this dataset consists of 400 gang member profiles and 2,865 non-gang member profiles",
      ". These gang-related terms are often local to gangs operating in neighborhoods and change rapidly when they form new gangs. Consequently, building a database of keywords, phrases, and other identifiers to find gang members nationally is not feasible. Instead, we use heterogeneous sets of features derived not only from profile and tweet text but also from the emoji usage, profile images, and links to YouTube videos reflecting their music preferences and affinity",
      ". These studies investigated a small set of manually curated gang member profiles, often from a small geographic area that may bias their findings. In our previous work BIBREF9 , we curated what may be the largest set of gang member profiles to study how gang member Twitter profiles can be automatically identified based on the content they share online"
    ]
  },
  {
    "title": "Recurrently Controlled Recurrent Networks",
    "answer": "RCRN and 3L-BiLSTM are approximately three times larger than BiLSTM.",
    "evidence": [
      ". Note that a single RCRN model is equivalent to a stacked BiLSTM of 3 layers. This is clear when we consider how two controller BiRNNs are used to control a single listener BiRNN. As such, for our experiments, when considering only the encoder and keeping all other components constant, 3L-BiLSTM has equal parameters to RCRN while RCRN and 3L-BiLSTM are approximately three times larger than BiLSTM.",
      ". This leads to an improvement of INLINEFORM0 on all four metrics. Note that our model only uses a single layered RCRN while R-NET uses 3 layered BiGRUs. This empirical evidence might suggest that RCRN is a better way to utilize multiple recurrent layers. Across all 26 datasets, RCRN outperforms not only standard BiLSTMs but also 3L-BiLSTMs which have approximately equal parameterization. 3L-BiLSTMs were overall better than BiLSTMs but lose out on a minority of datasets",
      ". Notably, the wide adoption of stacked architectures across many applications BIBREF7 , BIBREF8 , BIBREF9 , BIBREF10 signify the need for designing complex and expressive encoders. Unfortunately, these strategies may face limitations. For example, the former might run a risk of overfitting and/or hitting a wall in performance",
      ". In these models, the key idea is that the gating functions control information flow and compositionality over time, deciding how much information to read/write across time steps. This not only serves as a protection against vanishing/exploding gradients but also enables greater relative ease in modeling long-range dependencies. There are two common ways to increase the representation capability of RNNs. Firstly, the number of hidden dimensions could be increased",
      "Experiments This section discusses the overall empirical evaluation of our proposed RCRN model."
    ]
  },
  {
    "title": "Contextual Recurrent Units for Cloze-style Reading Comprehension",
    "answer": "The datasets used for testing sentiment classification are MR and IMDB, while for reading comprehension, the dataset used is not explicitly mentioned, but it is implied to be a modification of the AoA Reader.",
    "evidence": [
      "Experiments: Sentiment Classification ::: Experimental Setups In the sentiment classification task, we tried our model on the following public datasets. [leftmargin=*] MR Movie reviews with one sentence each. Each review is classified into positive or negative BIBREF18. IMDB Movie reviews from IMDB website, where each movie review is labeled with binary classes, either positive or negative BIBREF19. Note that each movie review may contain several sentences",
      ". In the sentiment classification task, we build a simple neural model and applied our CRU. In the cloze-style reading comprehension task, we first present some modifications to a recent reading comprehension model, called AoA Reader BIBREF10, and then replace the GRU part by our CRU model to see if our model could give substantial improvements over strong baselines.",
      "Experiments: Reading Comprehension ::: Results The overall experimental results are given in Table TABREF38. As we can see that our proposed models can substantially outperform various state-of-the-art systems by a large margin. [leftmargin=*] Overall, our final model (M-AoA Reader + CRU + Re-ranking) could give significant improvements over the previous state-of-the-art systems by 2.1% and 1.4% in test sets, while re-ranking and ensemble bring further improvements",
      "Applications ::: Reading Comprehension Besides the sentiment classification task, we also tried our CRU model in cloze-style reading comprehension, which is a much complicated task. In this paper, we strengthened the recent AoA Reader BIBREF10 and applied our CRU model to see if we could obtain substantial improvements when the baseline is strengthened.",
      "Applications ::: Sentiment Classification In the sentiment classification task, we aim to classify movie reviews, where one movie review will be classified into the positive/negative or subjective/objective category. A general neural network architecture for this task is depicted in Figure FIGREF20. First, the movie review is transformed into word embeddings. And then, a sequence modeling module is applied, in which we can adopt LSTM, GRU, or our CRU, to capture the inner relations of the text"
    ]
  },
  {
    "title": "Saliency Maps Generation for Automatic Text Summarization",
    "answer": "They compared their results to the baseline model from See et al. (2017) trained on the CNN/Daily Mail dataset.",
    "evidence": [
      ". We did however also try with different methods, like averaging the raw relevance or averaging a scaled absolute value where negative relevance is scaled down by a constant factor. The absolute value average seemed to deliver the best results. We delete incrementally the important words (words with the highest average) in the input and compared it to the control experiment that consists of deleting the least important word and compare the degradation of the resulting summaries",
      ". This however allows us to say that the attribution generated for the text at the origin of the summaries in Figure 4 are truthful in regard to the network's computation and we may use it for further studies of the example, whereas for the text at the origin of Figure 5 we shouldn't draw any further conclusions from the attribution generated",
      "The Task and the Model We present in this section the baseline model from See et al. See2017 trained on the CNN/Daily Mail dataset. We reproduce the results from See et al.",
      "Obtained Summaries We train the 21 350 992 parameters of the network for about 60 epochs until we achieve results that are qualitatively equivalent to the results of See et al. See2017. We obtain summaries that are broadly relevant to the text but do not match the target summaries very well. We observe the same problems such as wrong reproduction of factual details, replacing rare words with more common alternatives or repeating non-sense after the third sentence",
      ". We obtain mitigated results: for some texts, we observe a quick degradation when deleting important words which are not observed when deleting unimportant words (see Figure 4 ), but for other test examples we don't observe a significant difference between the two settings (see Figure 5 ). One might argue that the second summary in Figure 5 is better than the first one as it makes better sentences but as the model generates inaccurate summaries, we do not wish to make such a statement"
    ]
  },
  {
    "title": "Align, Mask and Select: A Simple Method for Incorporating Commonsense Knowledge into Language Representation Models",
    "answer": "They reserve four distractors and one correct answer for each question, and if there are less than four matched distractors, they discard the question instead of complementing it with random selection.",
    "evidence": [
      ". We mask the pronoun word with a special token [QW] to construct a question, and put the two candidate paragraphs as candidate answers. The remaining procedures are the same as QA tasks. We use the same loss function as BIBREF22 , that is, if c $_1$ is correct and c $_2$ is not, the loss is  $$\\begin{aligned} L = &- {\\rm logp}(c_1|s) + \\\\ &\\alpha \\cdot max(0, {\\rm logp}(c_2|s)-{\\rm logp}(c_1|s)+\\beta ), \\end{aligned}$$   (Eq",
      ". Some examples from CommonsenseQA are shown in Table 1 part A. As can be seen from the examples, although it is easy for humans to answer the questions based on their knowledge about the world, it is a great challenge for machines when there is limited training data. We hypothesize that exploiting knowledge graphs for commonsense in QA modeling can help model choose correct answers",
      ". For each question, we reserve four distractors and one correct answer. If there are less than four matched distractors, we discard this question instead of complementing it with random selection. If there are more than four distractors, we randomly select four distractors from them. After applying the AMS method, we create 16,324,846 multi-choice question answering samples.",
      ". However, collecting this dataset used a large amount of human efforts. In contrast, in this paper, we propose an “align, mask and select\" (AMS) method, inspired by the distant supervision approaches, to automatically construct a multi-choice question answering dataset.",
      ". Our model BERT_CS $_{large}$ chooses correct answers for both questions while BERT $_{large}$ chooses the same choice “Bill\" for both questions. We speculate that BERT $_{large}$ tends to choosing the closer candidates. We split WSC test set into two parts CLOSE and FAR according as the correct candidate is closer or farther to the pronoun word in the sentence than another candidate"
    ]
  },
  {
    "title": "Binary and Multitask Classification Model for Dutch Anaphora Resolution: Die/Dat Prediction",
    "answer": "The Europarl dataset contains 103,871 sentences, while the SoNaR dataset contains 1,269,091 sentences.",
    "evidence": [
      ". The Europarl dataset, on the one hand, contains 70,057 dat-labeled and 33,814 die-labeled sentences. The resulting train and test sets consist of 103,871 (Europarl) and 1,269,091 (SoNaR) sentences. The SoNaR dataset, on the other hand, has more than ten times the number of labeled sentences with 736,987 dat-labeled and 532,104 die-labeled",
      "Binary Classification Model ::: Results An overview of the performance results is given in Table TABREF11. We compare model performance when trained and tested on the two corpora individually and experiment with different settings of the two corpora in order to investigate the effect of dataset changes on model performance",
      ". Firstly, the data is divided into batch sizes of 512 instead of 128. Table TABREF22 shows, however, that there is little consistent difference in performance when batch size is 512 or 128. Therefore, it can be suggested that an increased batch size has no directly positive influence on model performance. Secondly, the input data is transformed to 200-dimensional word embeddings instead of 100-dimensional word embeddings",
      "Dataset The datasets used for training, validation and testing contain sentences extracted from the Europarl corpus BIBREF1 and SoNaR corpus BIBREF2. The Europarl corpus is an open-source parallel corpus containing proceedings of the European Parliament. The Dutch section consists of 2,333,816 sentences and 53,487,257 words. The SoNaR corpus comprises two corpora: SONAR500 and SONAR1. The SONAR500 corpus consists of more than 500 million words obtained from different domains",
      ". Therefore, it can be suggested that an increased batch size has no directly positive influence on model performance. Secondly, the input data is transformed to 200-dimensional word embeddings instead of 100-dimensional word embeddings. From the results displayed in Table TABREF22, it appears that a change in word embedding dimension could be causing an slight increase in model performance"
    ]
  },
  {
    "title": "Seeing Things from a Different Angle: Discovering Diverse Perspectives about Claims",
    "answer": "The challenges highlighted include determining argue-worthy claims, understanding argumentative structures, and discovering diverse perspectives and their supporting evidence with respect to a given claim.",
    "evidence": [
      ". We hope that some of these challenges and limitations will be addressed in future work.",
      "Design Principles and Challenges In this section we provide a closer look into the challenge and propose a collection of tasks that move us closer to substantiated perspective discovery. To clarify our description we use to following notation. Let INLINEFORM0 indicate a target claim of interest (for example, the claims INLINEFORM1 and INLINEFORM2 in Figure FIGREF6 ). Each claim INLINEFORM3 is addressed by a collection of perspectives INLINEFORM4 that are grouped into clusters of equivalent perspectives",
      ". In this work, we propose and study a setting that would facilitate discovering diverse perspectives and their supporting evidence with respect to a given claim. Our goal is to identify and formulate the key NLP challenges underlying this task, and develop a dataset that would allow a systematic study of these challenges",
      ". To ensure the diversity of responses, we do not require additional qualifications or demographic information from our annotators. For any of the annotations steps described below, the users are guided to an external platform where they first read the instructions and try a verification step to make sure they have understood the instructions. Only after successful completion are they allowed to start the annotation tasks",
      ". Creating systems that would address our challenge in its full glory requires solving the following interdependent tasks: Determination of argue-worthy claims: not every claim requires an in-depth discussion of perspectives. For a system to be practical, it needs to be equipped with understanding argumentative structures BIBREF3 in order to discern disputed claims from those with straightforward responses"
    ]
  },
  {
    "title": "Unsupervised Machine Commenting with Neural Variational Topic Model",
    "answer": "The dataset used is a large-scale Chinese dataset collected from Tencent News, consisting of 198,112 news articles with their corresponding comments.",
    "evidence": [
      ". The dataset consists of 198,112 news articles. Each piece of news contains a title, the content of the article, and a list of the users' comments. Following the previous work BIBREF0 , we tokenize all text with the popular python package Jieba, and filter out short articles with less than 30 words in content and those with less than 20 comments. The dataset is split into training/validation/test sets, and they contain 191,502/5,000/1,610 pieces of news, respectively",
      ". The candidate comment set consists of the following parts: Correct: The ground-truth comments of the corresponding news provided by the human. Plausible: The 50 most similar comments to the news. We use the news as the query to retrieve the comments that appear in the training set based on the cosine similarity of their tf-idf values. We select the top 50 comments that are not the correct comments as the plausible comments. Popular: The 50 most popular comments from the dataset",
      ". However, the naturally paired commenting dataset is loosely paired. Qin et al. QinEA2018 were the first to propose the article commenting task and an article-comment dataset. The dataset is crawled from a news website, and they sample 1,610 article-comment pairs to annotate the relevance score between articles and comments. The relevance score ranges from 1 to 5, and we find that only 6.8% of the pairs have an average score greater than 4",
      "Datasets We select a large-scale Chinese dataset BIBREF0 with millions of real comments and a human-annotated test set to evaluate our model. The dataset is collected from Tencent News, which is one of the most popular Chinese websites for news and opinion articles. The dataset consists of 198,112 news articles. Each piece of news contains a title, the content of the article, and a list of the users' comments",
      ". For the second issue, we bridge the articles and comments with their topics. Our model is based on a retrieval-based commenting framework, which uses the news as the query to retrieve the comments by the similarity of their topics. The topic is represented with a variational topic, which is trained in an unsupervised manner. The contributions of this work are as follows:"
    ]
  },
  {
    "title": "Learn to Code-Switch: Data Augmentation using Copy Mechanism on Language Modeling",
    "answer": "The paper does not explicitly mention the languages being explored, but it does provide an example of a code-switched sentence in Chinese (\"我们要去(We want to) check\"), suggesting that Chinese and possibly English are involved.",
    "evidence": [
      "Introduction Language mixing has been a common phenomenon in multilingual communities. It is motivated in response to social factors as a way of communication in a multicultural society. From a sociolinguistic perspective, individuals do code-switching in order to construct an optimal interaction by accomplishing the conceptual, relational-interpersonal, and discourse-presentational meaning of conversation BIBREF0",
      ". BIBREF13 added syntactic and semantic features to the Factorized RNN networks. BIBREF14 adapted an effective curriculum learning by training a network with monolingual corpora of both languages, and subsequently train on code-switched data. A further investigation of Equivalence Constraint and Curriculum Learning showed an improvement in language modeling BIBREF6 . A multi-task learning approach was introduced to train the syntax representation of languages by constraining the language generator BIBREF9",
      ". Combining the generated samples with code-switching dataset reduces perplexity. We get further performance gain after using syntactic information of the input. In future work, we plan to explore reinforcement learning for sequence generation and employ more parallel corpora.",
      ". There is a slight reduction in perplexity around 6 points on the test set. In addition, when we ignore the constraint, we lose performance because it still allows switches in the inversion grammar cases. Does the pointer-generator learn how to switch? We found that our pointer-generator model generates sentences that have not been seen before. The example in Figure FIGREF1 shows that our model is able to construct a new well-formed sentence such as “我们要去(We want to) check\"",
      ". This approach is the unification of all components in the work of BIBREF5 into a single computational model. A code-switching language model learned in this way is able to capture the patterns and constraints of the switches and mitigate the out-of-vocabulary (OOV) issue during sequence generation"
    ]
  },
  {
    "title": "Rnn-transducer with language bias for end-to-end Mandarin-English code-switching speech recognition",
    "answer": "They obtain language identities by adding language IDs in the CS point of transcription.",
    "evidence": [
      ". Compared with normal text, the tagged data can bias the RNN-T to predict language IDs in CS points. So our method can model the CS distribution directly, no additional LID model is needed. Then we constrain the input word embedding with its corresponding language ID, which is beneficial for model to learn the language identity information from transcription. In the inference process, the predicted language IDs are used to adjust the output posteriors",
      "Experiments setups ::: Dataset We conduct experiments on SEAME (South East Asia Mandarin English), a spontaneous conversational bilingual speech corpus BIBREF16. Most of the utterances contain both Mandarin and English uttered by interviews and conversations. We use the standard data partitioning rule of previous works which consists of three parts: $train$, $test_{sge}$ and $test_{man}$ (see Table 1) BIBREF2",
      ". And the RNN-T and attention-based models trained with large speech corpus perform competitively compared to the state-of-art model in some tasks BIBREF13. However, the lack of CS training data poses serious problem to end-to-end methods. To address the problem, language identity information is utilized to improve the performance of recognition BIBREF3, BIBREF4, BIBREF5. They are usually based on CTC or attention-based encoder-decoder models or the combination of both",
      ". In this work, we propose an improved recurrent neural network transducer (RNN-T) model with language bias to alleviate the problem. We use the language identities to bias the model to predict the CS points. This promotes the model to learn the language identity information directly from transcription, and no additional LID model is needed. We evaluate the approach on a Mandarin-English CS corpus SEAME. Compared to our RNN-T baseline, the proposed method can achieve 16.2% and 12",
      ". The model is trained to predict language IDs as well as the subwords. To ensure the model can learn CS information, we add language IDs in the CS point of transcription, as illustrated in Fig. In the figure, we use the arrangements of different geometric icons to represent the CS distribution. Compared with normal text, the tagged data can bias the RNN-T to predict language IDs in CS points. So our method can model the CS distribution directly, no additional LID model is needed"
    ]
  },
  {
    "title": "Fully Automated Fact Checking Using External Sources",
    "answer": "The task-specific embeddings are built using the last hidden layer of the NN for the claim and the average of the word embeddings in the text.",
    "evidence": [
      ". Finally, we combine the SVM with the NN by augmenting the input to the SVM with the values of the units in the hidden layer. This represents a task-specific embedding of the input example, and in our experiments it turned out to be quite helpful. Unlike in the SVM only model, this time we use the bi-LSTM embeddings as an input to the SVM. Ultimately, this yields a combination of deep learning and task-specific embeddings with RBF kernels.",
      ". We calculate these embeddings (i) as the average of the embeddings of the words in the text, and also (ii) using LSTM encodings, which we train for the task as part of a deep neural network (NN). We also use a task-specific embedding of the claim together with all the above evidence about it, which comes from the last hidden layer of the NN.",
      ". We further feed the network with the similarity features described above. All these vectors are concatenated and fully connected to a much more compact hidden layer that captures the task-specific embeddings. This layer is connected to a softmax output unit to classify the claim as true or false. The bottom of Figure FIGREF7 represents the generic architecture of each of the LSTM components",
      ". The claim is fed into the neural network as-is. As we can have multiple snippets, we only use the best-matching one as described above. Similarly, we only use a single best-matching triple of consecutive sentences from a Web page. We further feed the network with the similarity features described above. All these vectors are concatenated and fully connected to a much more compact hidden layer that captures the task-specific embeddings",
      ". Our second classifier is an SVM with an RBF kernel. The input is the same as for the NN: word embeddings and similarities. However, the word embeddings this time are calculated by averaging rather than using a bi-LSTM. Finally, we combine the SVM with the NN by augmenting the input to the SVM with the values of the units in the hidden layer. This represents a task-specific embedding of the input example, and in our experiments it turned out to be quite helpful"
    ]
  },
  {
    "title": "Aspect Term Extraction with History Attention and Selective Transformation",
    "answer": "They outperform state-of-the-art methods by a margin that allows them to surpass all of them.",
    "evidence": [
      ". After inserting the STN component before the bi-linear attention, i.e. “OURS w/o THA”, we get about 1% absolute gains on each dataset, and then the performance is comparable to CMLA. By adding THA, i.e. “OURS”, the performance is further improved, and all state-of-the-art methods are surpassed.",
      ". Another clue is the information of aspect detection history, and it is distilled from the previous aspect predictions so as to leverage the coordinate structure and tagging schema constraints to upgrade the aspect prediction. Experimental results over four benchmark datasets clearly demonstrate that our framework can outperform all state-of-the-art methods.",
      ". For example, it exploits the global opinion information by directly computing the association score between the aspect prototype and individual opinion hidden representations and then performing weighted aggregation. However, such aggregation may introduce noise. To some extent, this drawback is inherited from the attention mechanism, as also observed in machine translation BIBREF10 and image captioning BIBREF11",
      "Joint Training All the components in the proposed framework are differentiable. Thus, our framework can be efficiently trained with gradient methods",
      ". “OURS w/o THA & STN” only keeps the basic bi-linear attention. Although it performs not bad, it is still less competitive compared with the strongest baseline (i.e., CMLA), suggesting that only using attention mechanism to distill opinion summary is not enough. After inserting the STN component before the bi-linear attention, i.e. “OURS w/o THA”, we get about 1% absolute gains on each dataset, and then the performance is comparable to CMLA. By adding THA, i.e"
    ]
  },
  {
    "title": "Improving Visually Grounded Sentence Representations with Self-Attention",
    "answer": "The work is evaluated over well-known transfer tasks including movie review sentiment (MR), customer reviews (CR), subjectivity (SUBJ), opinion polarity (MPQA), paraphrase identification (MSRP), binary sentiment classification (SST), SICK entailment, and SICK relatedness, as well as the MS-COCO dataset.",
    "evidence": [
      "Introduction Recent NLP studies have thrived on distributional hypothesis. More recently, there have been efforts in applying the intuition to larger semantic units, such as sentences, or documents. However, approaches based on distributional semantics are limited by the grounding problem BIBREF0 , which calls for techniques to ground certain conceptual knowledge in perceptual information",
      ". We evaluate the quality of sentence representations produced from different variants of our encoders on well-known transfer tasks: movie review sentiment (MR) BIBREF32 , customer reviews (CR) BIBREF33 , subjectivity (SUBJ) BIBREF34 , opinion polarity (MPQA) BIBREF35 , paraphrase identification (MSRP) BIBREF36 , binary sentiment classification (SST) BIBREF37 , SICK entailment and SICK relatedness BIBREF38 .",
      ". A recent work proposed using supervised learning of a specific task as a leverage to obtain general sentence representation BIBREF7 . Convergence between computer vision and NLP researches have increasingly become common. Image captioning BIBREF8 , BIBREF9 , BIBREF10 , BIBREF11 and image synthesis BIBREF12 are two common tasks",
      "Evaluation Adhering to the experimental settings of BIBREF1 , we concatenate sentence representations produced from our model with those obtained from the state-of-the-art unsupervised learning model (Layer Normalized Skip-Thoughts, ST-LN) BIBREF31",
      ". We employ orthogonal initialization BIBREF28 for recurrent weights and xavier initialization BIBREF29 for all others. For the datasets, we use Karpathy and Fei-Fei's split for MS-COCO dataset BIBREF10 . Image features are prepared by extracting hidden representations at the final layer of ResNet-101 BIBREF30 . We evaluate sentence representation quality using SentEval BIBREF7 , BIBREF1 scripts. Mini-batch size is 128 and negative samples are prepared from remaining data samples in the same mini-batch."
    ]
  },
  {
    "title": "Deeper Task-Specificity Improves Joint Entity and Relation Extraction",
    "answer": "The variables in the ablation study were the inclusion of zero NER-specific BiRNN layers, zero RE-specific BiRNN layers, and zero task-specific BiRNN layers of any kind.",
    "evidence": [
      "Experiments ::: Ablation Study To further demonstrate the effectiveness of the additional task-specific BiRNN layers in our architecture, we conducted an ablation study using the CoNLL04 dataset. We trained and evaluated in the same manner described above, using the same hyperparameters, with the following exceptions: We used either (i) zero NER-specific BiRNN layers, (ii) zero RE-specific BiRNN layers, or (iii) zero task-specific BiRNN layers of any kind",
      ". An ablation study confirms the importance of the additional task-specific layers for achieving these results. Our work suggests that previous solutions to joint NER and RE undervalue task-specificity and demonstrates the importance of correctly balancing the number of shared and task-specific parameters for MTL approaches in general.",
      ". Table TABREF26 contains the results from the ablation study. These results show that the proposed architecture benefits from the inclusion of both NER- and RE-specific layers. However, the RE task benefits much more from the inclusion of these task-specific layers than does the NER task. We take this to reflect the fact that the RE task is more difficult than the NER task for the CoNLL04 dataset, and therefore benefits the most from its own task-specific layers",
      ". We increased the number of shared BiRNN layers to keep the total number of model parameters consistent with the number of parameters in the baseline model. We average the results for each set of hyperparameter across three trials with random weight initializations. Table TABREF26 contains the results from the ablation study. These results show that the proposed architecture benefits from the inclusion of both NER- and RE-specific layers",
      ". We suspect that this is due to the limited number of relations and entities in the ADE dataset. For most examples in this dataset, it is sufficient to correctly identify a single Drug entity, a single Adverse-Effect entity, and an Adverse-Effect relation between the two entities. Thus, the NER and RE tasks for this dataset are more closely related than they are in the case of the CoNLL04 dataset"
    ]
  },
  {
    "title": "Semi-supervised sequence tagging with bidirectional language models",
    "answer": "The evaluation datasets include the CoNLL 2003 test data, the CoNLL 2000 chunks, the PTB POS tags, the Chunking task, the NER task, and the SemEval 2017 Shared Task 10, ScienceIE.",
    "evidence": [
      ". Taken together, this suggests that for very small labeled training sets, transferring from other tasks yields a large improvement, but this improvement almost disappears when the training data is large. On the other hand, our approach is less dependent on the training set size and significantly improves performance even with larger training sets",
      ". Although we do not use external labeled data or gazetteers, we found that TagLM outperforms previous state of the art results in both tasks when external resources (labeled data or task specific gazetteers) are available. Furthermore, Tables TABREF17 and TABREF18 show that, in most cases, the improvements we obtain by adding LM embeddings are larger then the improvements previously obtained by adding other forms of transfer or joint learning. For example, BIBREF3 noted an improvement of only 0",
      ". Due to the much smaller size of this data set, we decreased the model size to 512 hidden units with a 256 dimension projection and normalized tokens in the same manner as input to the sequence tagging model (lower-cased, with all digits replaced with 0). The test set perplexities for the forward and backward models (measured on the CoNLL 2003 test data) were 106.9 and 104.2, respectively",
      ". For example, BIBREF3 noted an improvement of only 0.06 INLINEFORM0 in the NER task when transfer learning from both CoNLL 2000 chunks and PTB POS tags and BIBREF8 reported an increase of 0.71 INLINEFORM1 when adding gazetteers to their baseline. In the Chunking task, previous work has reported from 0.28 to 0.75 improvement in INLINEFORM2 when including supervised labels from the PTB POS tags or CoNLL 2003 entities BIBREF3 , BIBREF7 , BIBREF6 .",
      ". One artifact of our evaluation framework is that both the labeled data in the chunking and NER tasks and the unlabeled text in the 1 Billion Word Benchmark used to train the bidirectional LMs are derived from news articles. To test the sensitivity to the LM training domain, we also applied TagLM with a LM trained on news articles to the SemEval 2017 Shared Task 10, ScienceIE"
    ]
  },
  {
    "title": "Adapt or Get Left Behind: Domain Adaptation through BERT Language Model Finetuning for Aspect-Target Sentiment Classification",
    "answer": "The results show that language model finetuning has a substantial effect on ATSC end-task performance, with the performance starting to increase at about 10 Mio in the laptops domain and converging at around 17 Mio sentences, with no significant improvement after that.",
    "evidence": [
      "Experiments ::: Results Analysis The results of our experiments are shown in fig:acc-dep-lmiterations and tab:results respectively. To answer RQ1, which is concerned with details on domain-specific language model finetuning, we can see in fig:acc-dep-lmiterations that first of all, language model finetuning has a substantial effect on ATSC end-task performance. Secondly, we see that in the laptops domain the performance starts to increase at about 10 Mio",
      ". Secondly, we see that in the laptops domain the performance starts to increase at about 10 Mio. This is an interesting insight as one would expect a relation closer to a logarithmic curve. One reason might be that it takes many steps to train knowledge into the BERT language model due to its vast amount of parameters. The model already converges at around 17 Mio. sentences. More finetuning does not improve performance significantly",
      ". We train the model for a total of 7 epochs. The validation accuracy converges after about 3 epochs of training on all datasets, but training loss still improves after that. It is important to note that all our results reported are the average of 9 runs with different random initializations. This is needed to measure significance of improvements, as the standard deviation in accuray amounts to roughly $1\\%$ for all experiments, see fig:acc-dep-lmiterations.",
      ". The domain for training $D_{Train}$ can take on the same values, for the joint case case the training datasets for laptops and restaurants are simply combined. The domain for testing $D_{Test}$ can only be take on the values Restaurants or Laptops. Combining finetuning and training steps gives us nine different evaluation scenarios, which we group into the following four categories:",
      "Methodology ::: Domain Adaptation through Language Model Finetuning In academia, it is common that the performance of a machine learning model is evaluated in-domain. This means that the model is evaluated on a test set that comes from the same distribution as the training set. In real-world applications this setting is not always valid, as the trained model is used to predict previously unseen data"
    ]
  },
  {
    "title": "NumNet: Machine Reading Comprehension with Numerical Reasoning",
    "answer": "The existing models they compared with are the proposed numerically-aware graph and a traditional GNN, as well as a fully connected graph.",
    "evidence": [
      ". Above all, it is nontrivial to adapt most existing AWP models to the MRC scenario. Therefore, we focus on enhancing MRC models with numerical reasoning abilities in this work.",
      ". The “Comparison”, “Number” and “ALL” are corresponding to the comparing question subset , the number-type answer subset, and the entire development set, respectively . If we replace the proposed numerically-aware graph (Sec. SECREF19) with a fully connected graph, our model fallbacks to a traditional GNN, denoted as “GNN” in the table",
      ". In recent years, researchers have proposed lots of MRC models BIBREF0, BIBREF1, BIBREF2, BIBREF3 and these models have achieved remarkable results in various public benchmarks such as SQuAD BIBREF4 and RACE BIBREF5. The success of these models is due to two reasons: (1) Multi-layer architectures which allow these models to read the document and the question iteratively for reasoning; (2) Attention mechanisms which would enable these models to focus on the part related to the question in the document",
      ". However, all the existing AWP systems are only trained and validated on small benchmark datasets. BIBREF21 found that the performance of these AWP systems sharply degrades on larger datasets. Moreover, from the perspective of NLP, MRC problems are more challenging than AWP since the passages in MRC are mostly real-world texts which require more complex skills to be understood. Above all, it is nontrivial to adapt most existing AWP models to the MRC scenario",
      "Experiments ::: Experimental Settings In this paper, we tune our model on the development set and use a grid search to determine the optimal parameters. The dimensions of all the representations (e.g., $\\mathbf {Q}$, $\\mathbf {P}$, $\\mathbf {M}^Q$, $\\mathbf {M}^P$, $\\mathbf {U}$, $\\mathbf {M}_0^{\\prime }$, $\\mathbf {M}_0$ and $\\mathbf {v}$) are set to 128. If not specified, the reasoning step $K$ is set to 3"
    ]
  },
  {
    "title": "Classifying topics in speech when all you have is crummy translations.",
    "answer": "They look at Japanese and low-resource languages, with a specific example of a low-resource language being used in Uganda.",
    "evidence": [
      ". In all these studies, the query and search languages are the same, while we consider the cross-lingual case. There has been some limited work on cross-lingual keyword spotting BIBREF23, where ASR is cascaded with text-based cross-lingual retrieval. Some recent studies have attempted to use vision as a complementary modality to do cross-lingual retrieval BIBREF24, BIBREF25. But cross-lingual topic classification for speech has not been considered elsewhere, as far as we know.",
      "Introduction Quickly making sense of large amounts of linguistic data is an important application of language technology. For example, after the 2011 Japanese tsunami, natural language processing was used to quickly filter social media streams for messages about the safety of individuals, and to populate a person finder database BIBREF0. Japanese text is high-resource, but there are many cases where it would be useful to make sense of speech in low-resource languages",
      ". To report accuracy we compare the predicted labels and silver labels, i.e., we ask whether the topic inferred from our predicted translation (ST) agrees with one inferred from a gold translation (human).",
      ". As we are looking for topics which are coarse-level categories, we do not use the entire vocabulary, but instead focus only on the high importance terms. We lowercase the English translations and remove all punctuation, and stopwords. We further remove the terms occurring in more than 10% of the documents and those which occur in less than 2 documents, keeping only the 1000 most frequent out of the remaining",
      ". Japanese text is high-resource, but there are many cases where it would be useful to make sense of speech in low-resource languages. For example, in Uganda, as in many parts of the world, the primary source of news is local radio stations, which broadcast in many languages"
    ]
  },
  {
    "title": "LAXARY: A Trustworthy Explainable Twitter Analysis Model for Post-Traumatic Stress Disorder Assessment",
    "answer": "The Domain-Specific Risk-Taking (DOSPERT) Scale is the most popular and well-accepted clinically validated PTSD assessment tool mentioned in the context.",
    "evidence": [
      "Demographics of Clinically Validated PTSD Assessment Tools There are many clinically validated PTSD assessment tools that are being used both to detect the prevalence of PTSD and its intensity among sufferers. Among all of the tools, the most popular and well accepted one is Domain-Specific Risk-Taking (DOSPERT) Scale BIBREF15",
      ". In this paper, we develop LAXARY model where first we start investigating clinically validated survey tools which are trustworthy methods of PTSD assessment among clinicians, build our category sets based on the survey questions and use these as dictionary words in terms of first person singular number pronouns aspect for next level LIWC algorithm",
      ". As the clinical surveys are trusted and understandable method, we believe that this method will be able to gain trust of clinicians towards early detection of PTSD. Moreover, our proposed LAXARY model, which is first of its kind, can be used to develop any type of mental disorder Linguistic Dictionary providing a generalized and trustworthy mental health assessment framework of any kind.",
      ". In the context of the above research problem, we aim to answer the following research questions Given clinicians have trust on clinically validated PTSD assessment surveys, can we fill out PTSD assessment surveys using twitter posts analysis of war-veterans",
      ". LAXARY includes a modified LIWC model to calculate the possible scores of each survey question using PTSD Linguistic Dictionary to fill out the PTSD assessment surveys which provides a practical way not only to determine fine-grained discrimination of physiological and psychological health markers of PTSD without incurring the expensive and laborious in-situ laboratory testing or surveys, but also obtain trusts of clinicians who are expected to see traditional survey results of the PTSD assessment"
    ]
  },
  {
    "title": "Reference-less Quality Estimation of Text Simplification Systems",
    "answer": "The approaches compared include using reference-based MT metrics to evaluate TS systems, using MT evaluation metrics to compare TS system output with the source sentence, and various elementary metrics such as word, character, and syllable counts.",
    "evidence": [
      ". BIBREF20 also explored the idea of comparing the TS system output with its corresponding source sentence, but their metric, SARI, also requires to compare the output with a reference. In fact, this metric is designed to take advantage of more than one reference. It can be applied when only one reference is available for each source sentence, but its results are better when multiple references are available",
      ". They evaluate these metrics with respect to meaning preservation and grammaticality. Firstly, we extend the comparison to include the degree of simplicity achieved by the system. Secondly, we compare additional features, including those used by BIBREF18 , both individually, as elementary metrics, and within multi-feature metrics. To our knowledge, no previous work has provided as thorough a comparison across such a wide range and combination of features for the reference-less evaluation of TS",
      ". This creates a challenge for the use of reference-based MT metrics for TS evaluation. However, TS has the advantage of being a monolingual translation-like task, the source being in the same language as the output. This allows for new, non-conventional ways to use MT evaluation metrics, namely by using them to compare the output of a TS system with the source sentence, thus avoiding the need for reference data",
      ". Very simple features such as the percentage of words in common between source and output also rank high. Surprisingly, word embedding comparison methods do not perform as well for meaning preservation, even when using word alignment. Methods that give the best results are the most straightforward for assessing simplicity, namely word, character and syllable counts in the output, averaged over the number of output sentences",
      ". TABLE TABREF30 lists 30 of the elementary metrics that we compared, which are those that we found to correlate the most with human judgments on one or more of the three dimensions (grammaticality, meaning preservation, simplicity)."
    ]
  },
  {
    "title": "A Deep Neural Architecture for Sentence-level Sentiment Classification in Twitter Social Networking",
    "answer": "The Stanford Twitter Sentiment Corpus, the Sander dataset, and the Health Care Reform (HCR) dataset are used for experiments.",
    "evidence": [
      "Experimental setups For the Stanford Twitter Sentiment Corpus, we use the number of samples as BIBREF5 . The training data is selected 80K tweets for a training data and 16K tweets for the development set randomly from the training data of BIBREF0 . We conduct a binary prediction for STS Corpus. For Sander dataset, we use standard 10-fold cross validation as BIBREF14 . We construct the development set by selecting 10% randomly from 9-fold training data",
      "Introduction Twitter sentiment classification have intensively researched in recent years BIBREF0 BIBREF1 . Different approaches were developed for Twitter sentiment classification by using machine learning such as Support Vector Machine (SVM) with rule-based features BIBREF2 and the combination of SVMs and Naive Bayes (NB) BIBREF3 . In addition, hybrid approaches combining lexicon-based and machine learning methods also achieved high performance described in BIBREF4",
      ". However, it has been widely used in different evaluation tasks BIBREF0 BIBREF5 BIBREF13 . Sanders - Twitter Sentiment Corpus: This dataset consists of hand-classified tweets collected by using search terms: INLINEFORM0 , #google, #microsoft and #twitter. We construct the dataset as BIBREF14 for binary classification. Health Care Reform (HCR): This dataset was constructed by crawling tweets containing the hashtag #hcr BIBREF15 . Task is to predict positive/negative tweets BIBREF14 .",
      "Data Preparation Stanford - Twitter Sentiment Corpus (STS Corpus): STS Corpus contains 1,600K training tweets collected by a crawler from BIBREF0 . BIBREF0 constructed a test set manually with 177 negative and 182 positive tweets. However, it has been widely used in different evaluation tasks BIBREF0 BIBREF5 BIBREF13 . Sanders - Twitter Sentiment Corpus: This dataset consists of hand-classified tweets collected by using search terms: INLINEFORM0 , #google, #microsoft and #twitter",
      "Experimental results Table IV shows the result of our model for sentiment classification against other models. We compare our model performance with the approaches of BIBREF0 BIBREF5 on STS Corpus. BIBREF0 reported the results of Maximum Entropy (MaxEnt), NB, SVM on STS Corpus having good performance in previous time. The model of BIBREF5 is a state-of-the-art so far by using a CharSCNN. As can be seen, 86.63 is the best prediction accuracy of our model so far for the STS Corpus"
    ]
  },
  {
    "title": "Bleaching Text: Abstract Features for Cross-lingual Gender Prediction",
    "answer": "The evaluation metrics used include Fleiss kappa for inter-annotator agreement and accuracy against gold labels.",
    "evidence": [
      "Acknowledgments We would like to thank the three anonymous reviewers and our colleagues for their useful feedback on earlier versions of this paper. Furthermore, we are grateful to Chloé Braud for helping with the French human evaluation part. We would like to thank all of our human participants.",
      ". Each user was represented by twenty tweets. The answer key (F/M) order was randomized. For each of the three experiments we had six judges, balanced for gender, and obtained three annotations per target user. Inter-annotator agreement for the tasks was measured via Fleiss kappa ( INLINEFORM0 ), and was higher for the in-language experiment ( INLINEFORM1 ) than for the cross-language tasks (NL INLINEFORM2 PT: INLINEFORM3 ; FR INLINEFORM4 NL: INLINEFORM5 )",
      ". The latter setting is also used for the embeddings model. Table TABREF13 shows results for both the cross-language and in-language experiments in the lexical and abstract-feature setting. Within language, the lexical features unsurprisingly work the best, achieving an average accuracy of 80.5% over all languages. The abstract features lose some information and score on average 11.8% lower, still beating the majority baseline (50%) by a large margin (68.7%)",
      ". Inter-annotator agreement for the tasks was measured via Fleiss kappa ( INLINEFORM0 ), and was higher for the in-language experiment ( INLINEFORM1 ) than for the cross-language tasks (NL INLINEFORM2 PT: INLINEFORM3 ; FR INLINEFORM4 NL: INLINEFORM5 ). Table TABREF22 shows accuracy against the gold labels, comparing humans (average accuracy over three annotators) to lexical and bleached models on the exact same subset of 200 users",
      ". Lexicalized models are also superior to the bleached ones, as already seen on the full datasets (Table TABREF13 ). We can also observe that the amount of information available to represent a user influences system's performance. Training on 200 tweets per user, but testing on 20 tweets only, decreases performance by 12 percentage points. This is likely due to the fact that inputs are sparser, especially since the bleached model is trained on 5-grams"
    ]
  },
  {
    "title": "Community Question Answering Platforms vs. Twitter for Predicting Characteristics of Urban Neighbourhoods",
    "answer": "They try to predict 62 demographic attributes.",
    "evidence": [
      ". In this work, we look at a wide range of attributes and report prediction results on 62 demographic attributes. Additionally, work in urban prediction uses geolocation-based platforms such as Twitter. QA data that has been utilised in this paper does not include geolocation information. Utilising such data presents its own challenges.",
      ". In the Appendix, in Tables 6 and 7 , we show the prediction results for a wide range of 62 demographic attributes using Yahoo! For each attribute, we display two terms with the highest coefficient common between the majority of the folds. Attributes are divided into categories such as Religion, Employment, Education, etc. Answers performs slightly better than Twitter with an average $1\\%$ increase over all the attributes",
      "Prediction We investigate how well the demographic attributes can be predicted by using using Yahoo! We define the task of predicting a continuous-valued demographic attribute for unseen neighbourhoods as a regression task given their normalised tf-idf document representation. A separate regression task is defined for each demographic attribute. We choose linear regression for the prediction tasks as it has been widely used for predictions from text in the literature BIBREF10 , BIBREF11",
      ". We experiment with predicting a wide range of 62 demographic attributes for neighbourhoods of London. We use the text from QA platform of Yahoo! Answers and compare our results to the ones obtained from Twitter microblogs. Outcomes show that the correlation between the predicted demographic attributes using text from Yahoo! Answers discussions and the observed demographic attributes can reach an average Pearson correlation coefficient of \\r{ho} = 0",
      ". Answers data to the performance of data from Twitter, a platform that has been widely used for predicting many real-world attributes. Unlike many current works that focus on predicting one or few selected attributes (e.g. deprivation, race or income) using social media data, we study a wide range of 62 demographic attributes"
    ]
  },
  {
    "title": "Bias in Semantic and Discourse Interpretation",
    "answer": "The paper analyzes interpretive bias in the context of histories, which are discursive interpretations of a set of data, and in the process of interpreting a text or building a history over a set of facts.",
    "evidence": [
      "Organization of the paper Our paper is organized as follows. Section SECREF2 introduces our model of interpretive bias. Section SECREF3 looks forward towards some consequences of our model for learning and interpretation. We then draw some conclusions in Section SECREF4 . A detailed and formal analysis of interpretive bias has important social implications",
      ". Our model can show us what such a bias looks like. And we will examine the question of whether it is possible to find such a truth oriented bias for a set of facts, and if so, under what conditions. Can we detect and avoid biases that don't get at the truth but are devised for some other purpose? Our study of interpretive bias relies on three key premises",
      ". Can we detect and avoid biases that don't get at the truth but are devised for some other purpose? Our study of interpretive bias relies on three key premises. The first premise is that histories are discursive interpretations of a set of data in the sense that like discourse interpretations, they link together a set of entities with semantically meaningful relations. As such they are amenable to an analysis using the tools used to model a discourse's content and structure",
      "The model of interpretive bias As mentioned in Section SECREF1 , understanding interpretive bias requires two ingredients. First, we need to know what it is to interpret a text or to build a history over a set of facts. Our answer comes from analyzing discourse structure and interpretation in SDRT BIBREF2 , BIBREF3",
      ". Outside of language, statisticians study bias; and sample bias is currently an important topic. To do so, they exploit statistical models with a set of parameters and random variables, which play the role of our types in interpretive bias. But for us, the interpretive process is already well underway once the model, with its constraints, features and explanatory hypotheses, is posited; at least a partial history, or set of histories, has already been created"
    ]
  },
  {
    "title": "Spatial Concept Acquisition for a Mobile Robot that Integrates Self-Localization and Unsupervised Word Discovery from Spoken Sentences",
    "answer": "They use three types of word segmentation methods, including an unsupervised morphological analyzer capable of using lattices, and two other methods are not specified in detail.",
    "evidence": [
      "Phoneme recognition accuracy of uttered sentences We compared the performance of three types of word segmentation methods for all the considered uttered sentences. It was difficult to weigh the ambiguous syllable recognition and the unsupervised word segmentation separately. Therefore, this experiment considered the positions of a delimiter as a single letter",
      ". These results and the results reported in Section SECREF49 suggest that learning by uttered sentences obtained by better phoneme recognition and better word segmentation produces a good result for the acquisition of spatial concepts. Furthermore, in a comparison of two clustering methods, we found that SpCoA was considerably better than DPM, a word clustering method without location information, irrespective of the word segmentation method used",
      ". Table TABREF55 presents examples of the word segmentation results of the three considered methods. We found that the unsupervised morphological analyzer capable of using lattices improved the accuracy of phoneme recognition and word segmentation. Consequently, this result suggests that this word segmentation method considers the multiple hypothesis of speech recognition as a whole and reduces uncertainty such as variability in recognition by using the syllable recognition results in the lattice format.",
      ". on the basis of the integration of the learning of object concepts with a language model BIBREF34 , BIBREF35 . Following a similar approach, Heymann et al. proposed a method that alternately and repeatedly updates phoneme recognition results and the language model by using unsupervised word segmentation BIBREF36 . As a result, they achieved robust lexical acquisition",
      ". The Julius system uses a word dictionary containing 115 Japanese syllables. The microphone attached on the robot is SHURE's PG27-USB. Further, an unsupervised morphological analyzer, a latticelm 0.4, is implemented BIBREF22 . In the experiment, we compare the following three types of word segmentation methods. A set of syllable sequences is given to the graphical model of SpCoA by each method. This set is used for the learning of spatial concepts as recognized uttered sentences INLINEFORM0"
    ]
  },
  {
    "title": "Localization of Fake News Detection via Multitask Transfer Learning",
    "answer": "The dataset is composed of 3,206 news articles, with a 50/50 split between 1,603 real and fake articles.",
    "evidence": [
      "Experimental Setup ::: Fake News Dataset We work with a dataset composed of 3,206 news articles, each labeled real or fake, with a perfect 50/50 split between 1,603 real and fake articles, respectively. Fake articles were sourced from online sites that were tagged as fake news sites by the non-profit independent media fact-checking organization Verafiles and the National Union of Journalists in the Philippines (NUJP)",
      ". For training, we use a learning rate of 1e-4 and a batch size of 256. We train the model for 1,000,000 steps with 10,000 steps of learning rate warmup for 157 hours on a Google Cloud Tensor processing Unit (TPU) v3-8. For GPT-2, we pretrain a GPT-2 Transformer model on our prepared text corpora using language modeling as its sole pretraining task, according to the specifications of BIBREF8. We use an embedding dimension of 410, a hidden dimension of 2100, and a maximum sequence length of 256",
      ".” In addition, BERT also benefits from being deep, allowing it to capture more context and information. BERT-Base, the smallest BERT model, has 12 layers (768 units in each hidden layer) and 12 attention heads for a total of 110M parameters. Its larger sibling, BERT-Large, has 24 layers (1024 units in each hidden layer) and 16 attention heads for a total of 340M parameters.",
      ". We use an embedding dimension of 410, a hidden dimension of 2100, and a maximum sequence length of 256. We use 10 attention heads per multihead attention block, with 16 blocks composing the encoder of the transformer. We use dropout on all linear layers to a probability of 0.1. We initialize all parameters to a standard deviation of 0.02. For training, we use a learning rate of 2.5e-4, and a batch size of 32, much smaller than BERT considering the large size of the model",
      ". Both BIBREF1 and BIBREF3 use the Fake News Challenge dataset, with 49,972 labeled stances for each headline-body pairs. BIBREF2, on the other hand, uses the LIAR dataset BIBREF4, which contains 12,836 labeled short statements as well as sources to support the labels. This requirement for large datasets to effectively train fake news detection models from scratch makes it difficult to adapt these techniques into low-resource languages"
    ]
  },
  {
    "title": "Learning Word Embeddings from the Portuguese Twitter Stream: A Study of some Practical Aspects",
    "answer": "The consistent increase in the validation loss after about 15 epochs when using less than 50% of the data available indicates that the model tends to overfit the data.",
    "evidence": [
      ". For a fixed INLINEFORM0 = 32768 we varied the amount of data used for training from 25% to 100%. As we train with more data, we obtain better validation losses. The second trend is that by using less than 50% of the data available the model tends to overfit the data, as indicated by the consistent increase in the validation loss after about 15 epochs (check dashed lines in right side of Figure FIGREF28 )",
      ". The second trend is that by using less than 50% of the data available the model tends to overfit the data, as indicated by the consistent increase in the validation loss after about 15 epochs (check dashed lines in right side of Figure FIGREF28 ). This suggests that for the future we should not try any drastic reduction of the training data to save training time. Finally, when not overfitting, the validation loss seems to stabilize after around 20 epochs",
      ". On the right side of Figure FIGREF28 we show how the number of training (and validation) examples affects the loss. For a fixed INLINEFORM0 = 32768 we varied the amount of data used for training from 25% to 100%. As we train with more data, we obtain better validation losses",
      ". This suggests that for the future we should not try any drastic reduction of the training data to save training time. Finally, when not overfitting, the validation loss seems to stabilize after around 20 epochs. We observed no phase-transition effects (the model seems simple enough for not showing that type of behavior). This indicates we have a practical way of safely deciding when to stop training the model.",
      ". When using 100% we see no model overfitting. We can also observe that the higher is INLINEFORM1 the higher are the absolute values of the loss sets. This is not surprising because as the number of words to predict becomes higher the problem will tend to become harder. Also, because we keep the dimensionality of the embedding space constant (64 dimensions), it becomes increasingly hard to represent and differentiate larger vocabularies in the same hyper-volume"
    ]
  }
]